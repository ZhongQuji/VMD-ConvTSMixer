{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03a116c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T11:13:13.165916Z",
     "start_time": "2023-05-07T11:13:13.155914Z"
    }
   },
   "outputs": [],
   "source": [
    "# 调用相关库\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from scipy.io import savemat,loadmat\n",
    "import numpy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b589f156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T11:11:04.113955Z",
     "start_time": "2023-05-07T11:11:04.107953Z"
    }
   },
   "outputs": [],
   "source": [
    "# 转换成监督数据，将每个特征转换成21列数据，20->1，20组预测一组\n",
    "def series_to_supervised(data, n_in=20, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]  #计算特征的数量\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    # 将20组输入数据依次向下移动20,19,...3,2,1行，将数据加入cols列表（技巧：(n_in, 0, -1)中的-1指倒序循环，步长为1）\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    # 将一组输出数据加入cols列表（技巧：其中i=0）\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # cols列表(list)中现在有21块经过下移后的数据(即：df(-20),df(-19),...df(-2),df(-1),df)，将四块数据按列 并排合并\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    # 给合并后的数据添加列名\n",
    "    agg.columns = names\n",
    "    # print(agg)\n",
    "    # 删除NaN值列\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af5e2939",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T11:11:08.504999Z",
     "start_time": "2023-05-07T11:11:07.901864Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_excel(\"816480.xlsx\")\n",
    "fratures_Corrected_irradiance = [\n",
    "    'Flow (Veh/5 Minutes)'\n",
    "]\n",
    "values = dataset[fratures_Corrected_irradiance].values\n",
    "n_features=1 #特征数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1943f405",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T11:11:09.049133Z",
     "start_time": "2023-05-07T11:11:09.041141Z"
    }
   },
   "outputs": [],
   "source": [
    "# 标准化\n",
    "scaler =StandardScaler()\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8e9b3d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T11:11:09.527266Z",
     "start_time": "2023-05-07T11:11:09.519265Z"
    }
   },
   "outputs": [],
   "source": [
    "n_in = 3\n",
    "n_out = 1\n",
    "# 构造一个20->1的监督学习型数据\n",
    "reframed = series_to_supervised(scaled, n_in=n_in, n_out=n_out)\n",
    "values=reframed.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f0a5bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T11:11:10.517500Z",
     "start_time": "2023-05-07T11:11:10.510498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1149, 3, 1) (1149, 1) (288, 3, 1) (288, 1)\n"
     ]
    }
   ],
   "source": [
    "#训练集和测试集的划分比例\n",
    "ratio=0.8 \n",
    "N=int(len(values) *ratio)\n",
    "train = values[:N, :]\n",
    "test = values[N:, :]\n",
    "\n",
    "# 划分输入和输出,split into input and outputs\n",
    "n_obs = n_in *  n_features\n",
    "train_X, train_y = train[:, :n_obs], train[:, n_obs:]\n",
    "test_X, test_y = test[:, :n_obs], test[:, n_obs:]\n",
    "\n",
    "# 将数据转换为3D输入，[samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_in, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_in, n_features))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88636a2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T11:13:16.460765Z",
     "start_time": "2023-05-07T11:13:16.452764Z"
    }
   },
   "outputs": [],
   "source": [
    "# 注意力机制\n",
    "class AttentionLayer(Layer):\n",
    "    # BahdanauAttention https://blog.csdn.net/u010960155/article/details/82853632\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        # W.shape = (time_steps, time_steps)\n",
    "        self.W = self.add_weight(name='att_weight',\n",
    "                                 shape=(input_shape[1], input_shape[1]),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias',\n",
    "                                 shape=(input_shape[1],),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):  # 输入：inputs.shape = (batch_size, time_steps, seq_len)\n",
    "        # Permute将2、1轴翻转后，(batch_size, time_steps, seq_len) -> (batch_size, lstm_units, seq_len)\n",
    "        # 转换后：x.shape = (batch_size, seq_len, time_steps)\n",
    "        x = K.permute_dimensions(inputs, (0, 2, 1))\n",
    "\n",
    "        # 经过一个全连接层和Softmax后，其维度仍为(batch_size, seq_len, time_steps)\n",
    "        # 其实际内涵为，利用全连接层计算每一个time_steps的权重\n",
    "        a = K.softmax(K.tanh(K.dot(x, self.W) + self.b))\n",
    "\n",
    "        # a * x后获得每一个step中，每个维度在所有step中的权重\n",
    "        # 再经过Permute将2、1轴翻转后，(batch_size, seq_len, time_steps) -> (batch_size, time_steps, seq_len)\n",
    "        outputs = K.permute_dimensions(a * x, (0, 2, 1))\n",
    "        outputs = K.sum(outputs, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08039c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T11:15:11.659205Z",
     "start_time": "2023-05-07T11:15:11.645202Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "进行适应度计算,以均方差为适应度函数，目的是找到一组超参数 使得网络的误差最小\n",
    "'''\n",
    "#  这里给你优化了学习率和lstm神经元个数两个参数   后期想加可以增加去优化迭代次数，batch_size等参数   ，lstm其他层数的神经元   这里网络是但lstm层   优化的参数越多相对来说算法运行时间越长，也和下边的max_iter和population_size有关\n",
    "def fitness(pop, P, T, Pt, Tt):\n",
    "    tf.random.set_seed(0)\n",
    "    alpha = pop[0]  # 学习率\n",
    "    hidden_nodes0 = int(pop[1])  # 第一隐含层神经元\n",
    "    #     num_epochs = int(pop[2])#迭代次数\n",
    "    #     batch_size = int(pop[3])# batchsize\n",
    "    #     hidden_nodes = int(pop[4])#第二隐含层神经元\n",
    "\n",
    "    inputs = Input(shape=(train_X.shape[1], train_X.shape[2]))\n",
    "    lstm = LSTM(hidden_nodes0, activation='selu', return_sequences=True)(inputs)\n",
    "    # 注意力机制\n",
    "    attention = AttentionLayer()(lstm)\n",
    "    outputs = Dense(1)(attention)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha), loss='mse')\n",
    "    model.summary()  # 展示模型结构\n",
    "\n",
    "    model.fit(train_X, train_y, epochs=60, batch_size=30, validation_data=(test_X, test_y), verbose=2,\n",
    "              shuffle=False)\n",
    "\n",
    "    # 对测试结果进行反归一化\n",
    "    test_pred = model.predict(test_X)\n",
    "\n",
    "    F2 = np.mean(np.square((test_pred-test_y)))\n",
    "\n",
    "    return F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bead972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T11:19:20.491358Z",
     "start_time": "2023-05-07T11:19:20.476355Z"
    }
   },
   "outputs": [],
   "source": [
    "def boundary(pop,lb,ub):\n",
    "    # 防止粒子跳出范围,除学习率之外 其他的都是整数\n",
    "    pop=[int(pop[i]) if i>0 else pop[i] for i in range(len(lb))]\n",
    "    for i in range(len(lb)):\n",
    "        if pop[i]>ub[i] or pop[i]<lb[i]:\n",
    "            if i==0:\n",
    "                pop[i] = (ub[i]-lb[i])*np.random.rand()+lb[i]\n",
    "            else:\n",
    "                pop[i] = np.random.randint(lb[i],ub[i])\n",
    "    return pop\n",
    "\n",
    "def IPSO(train_X, train_Y, valid_X, valid_Y):\n",
    "    # PSO参数设置\n",
    "    pN = 5    # pN 最大迭代次数\n",
    "    max_iter = 10  #pN#max_iter 和pN这两个参数设置的越大,相对来说寻优出来适应度越好效果越好,但是算法运行花的时间就越多   这里给你设置的都很小 ，为了快速跑出结\n",
    "    #优化了两个参数  学习率和神经元个数   Lb和Ub分别为寻优范围上下限\n",
    "    # 第一个是学习率[0.001 0.01]\n",
    "    # 第二个是神经元个数[10-120]\n",
    "    lb = [0.001,10]\n",
    "    ub = [0.01,100]#\n",
    "    dim = len(lb)  # 寻优维度\n",
    "    \n",
    "    c1 = 1.5;\n",
    "    c2 = 1.5;\n",
    "    r1 = 0.8;\n",
    "    r2 = 0.3\n",
    "    wmax = 0.8;\n",
    "    wmin = 0.6\n",
    "\n",
    "    # 初始化\n",
    "    X = np.zeros((pN, dim))\n",
    "    V = np.zeros((pN, dim))\n",
    "    pbest = np.zeros((pN, dim))\n",
    "    gbest = np.zeros((1, dim))\n",
    "    p_fit = np.zeros(pN)\n",
    "    result = np.zeros((max_iter, dim))\n",
    "    fit = np.inf\n",
    "    for i in range(pN):\n",
    "        for j in range(dim):\n",
    "            if j == 0:  # 学习率是小数 其他的是整数\n",
    "                X[i][j] = (ub[j] - lb[j]) * np.random.rand() + lb[j]\n",
    "            else:\n",
    "                X[i][j] = np.random.randint(lb[j], ub[j])\n",
    "            V[i][j] = np.random.rand()\n",
    "        pbest[i] = X[i].copy()\n",
    "        tmp = fitness(X[i, :], train_X, train_Y, valid_X, valid_Y)\n",
    "        p_fit[i] = tmp\n",
    "        if (tmp < fit):\n",
    "            fit = tmp\n",
    "            gbest = X[i]\n",
    "            # 开始循环迭代\n",
    "    trace = []\n",
    "    for t in range(max_iter):\n",
    "\n",
    "        w = wmax - (wmax - wmin) * np.tanh(np.pi / 4 * t / max_iter)\n",
    "\n",
    "        for i in range(pN):\n",
    "            V[i, :] = w * V[i, :] + c1 * r1 * (pbest[i] - X[i, :]) + c2 * r2 * (gbest - X[i, :])\n",
    "            X[i, :] = X[i, :] + V[i, :]\n",
    "            X[i, :] = boundary(X[i, :], lb, ub)  # 边界判断\n",
    "            # 加入自适应变异操作，避免陷入局部最优\n",
    "            prob = 0.5 * t / max_iter + 0.5  # 自适应变异，随着进化代数的增加，变异几率越小\n",
    "            if np.random.rand() > prob:\n",
    "                for j in range(dim):\n",
    "                    if j == 0:\n",
    "                        X[i][j] = (ub[j] - lb[j]) * np.random.rand() + lb[j]\n",
    "                    else:\n",
    "                        X[i][j] = np.random.randint(lb[j], ub[j])\n",
    "\n",
    "        for i in range(pN):  # 更新gbest\\pbest\n",
    "            temp = fitness(X[i, :], train_X, train_Y, valid_X, valid_Y)\n",
    "            if (temp < p_fit[i]):  # 更新个体最优\n",
    "                p_fit[i] = temp\n",
    "                pbest[i, :] = X[i, :]\n",
    "                if (p_fit[i] < fit):  # 更新全局最优\n",
    "                    gbest = X[i, :].copy()\n",
    "                    fit = p_fit[i].copy()\n",
    "        result[t, :] = gbest.copy()\n",
    "        trace.append(fit)\n",
    "        print(t, fit, [int(gbest[i]) if i > 0 else gbest[i] for i in range(len(lb))])\n",
    "    return trace, gbest, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3516dc21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T11:20:02.453691Z",
     "start_time": "2023-05-07T11:19:23.034036Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 3, 23)             2300      \n",
      "                                                                 \n",
      " attention_layer (AttentionL  (None, 23)               12        \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 24        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,336\n",
      "Trainable params: 2,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 3s - loss: 0.3870 - val_loss: 0.0733 - 3s/epoch - 83ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0585 - val_loss: 0.0623 - 298ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0544 - val_loss: 0.0602 - 322ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0527 - val_loss: 0.0582 - 314ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0518 - val_loss: 0.0583 - 315ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0553 - val_loss: 0.0594 - 347ms/epoch - 9ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0514 - val_loss: 0.0539 - 298ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0471 - val_loss: 0.0526 - 323ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0471 - val_loss: 0.0537 - 331ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0494 - val_loss: 0.0529 - 323ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0460 - val_loss: 0.0499 - 314ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0438 - val_loss: 0.0494 - 298ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0441 - val_loss: 0.0496 - 303ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0441 - val_loss: 0.0482 - 281ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0421 - val_loss: 0.0471 - 297ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0414 - val_loss: 0.0470 - 279ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0420 - val_loss: 0.0469 - 289ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0418 - val_loss: 0.0462 - 441ms/epoch - 11ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0416 - val_loss: 0.0470 - 313ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0413 - val_loss: 0.0454 - 303ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0448 - 338ms/epoch - 9ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0450 - 312ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0444 - 303ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0441 - 302ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0442 - 301ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0438 - 290ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0437 - 314ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0435 - 313ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0435 - 306ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0433 - 323ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0433 - 322ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0431 - 330ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0430 - 314ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0429 - 319ms/epoch - 8ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0429 - 312ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0429 - 349ms/epoch - 9ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 338ms/epoch - 9ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 304ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0428 - 312ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 294ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0427 - 357ms/epoch - 9ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0428 - 344ms/epoch - 9ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 436ms/epoch - 11ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 397ms/epoch - 10ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 347ms/epoch - 9ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 420ms/epoch - 11ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 375ms/epoch - 10ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 343ms/epoch - 9ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 375ms/epoch - 10ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 342ms/epoch - 9ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 344ms/epoch - 9ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 322ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 354ms/epoch - 9ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 335ms/epoch - 9ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 340ms/epoch - 9ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 355ms/epoch - 9ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 460ms/epoch - 12ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 396ms/epoch - 10ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 414ms/epoch - 11ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0426 - 401ms/epoch - 10ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 3, 57)             13452     \n",
      "                                                                 \n",
      " attention_layer_1 (Attentio  (None, 57)               12        \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 58        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,522\n",
      "Trainable params: 13,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2390 - val_loss: 0.0779 - 1s/epoch - 35ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0581 - val_loss: 0.0585 - 367ms/epoch - 9ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0522 - val_loss: 0.0566 - 362ms/epoch - 9ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0541 - val_loss: 0.0649 - 356ms/epoch - 9ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0673 - val_loss: 0.0551 - 461ms/epoch - 12ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0506 - val_loss: 0.0543 - 346ms/epoch - 9ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0544 - val_loss: 0.0615 - 345ms/epoch - 9ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0596 - val_loss: 0.0569 - 353ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0551 - val_loss: 0.0543 - 404ms/epoch - 10ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0481 - val_loss: 0.0502 - 317ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0451 - val_loss: 0.0500 - 288ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0439 - val_loss: 0.0498 - 325ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0514 - val_loss: 0.0596 - 370ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0500 - val_loss: 0.0478 - 406ms/epoch - 10ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0506 - val_loss: 0.0621 - 492ms/epoch - 13ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0479 - val_loss: 0.0448 - 378ms/epoch - 10ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0441 - 382ms/epoch - 10ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0440 - 313ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0430 - 327ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0448 - 322ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0436 - 383ms/epoch - 10ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0439 - 365ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0428 - 300ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0432 - 295ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0429 - 291ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0434 - 302ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0435 - 420ms/epoch - 11ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0435 - 273ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0432 - 287ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0431 - 305ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0431 - 310ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0433 - 310ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0437 - 323ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0435 - 344ms/epoch - 9ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0433 - 295ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0433 - 321ms/epoch - 8ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0434 - 337ms/epoch - 9ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0435 - 301ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0436 - 419ms/epoch - 11ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0436 - 302ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0438 - 407ms/epoch - 10ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0431 - 352ms/epoch - 9ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0433 - 291ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0438 - 295ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0432 - 298ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0433 - 288ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0434 - 282ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0437 - 300ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0432 - 296ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0434 - 296ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0435 - 352ms/epoch - 9ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0433 - 361ms/epoch - 9ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0431 - 320ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0439 - 296ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0441 - 289ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0434 - 296ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0431 - 344ms/epoch - 9ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0435 - 440ms/epoch - 11ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0435 - 321ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0431 - 339ms/epoch - 9ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 3, 69)             19596     \n",
      "                                                                 \n",
      " attention_layer_2 (Attentio  (None, 69)               12        \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 70        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,678\n",
      "Trainable params: 19,678\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.3755 - val_loss: 0.0686 - 1s/epoch - 35ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0581 - val_loss: 0.0639 - 344ms/epoch - 9ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 1s - loss: 0.0558 - val_loss: 0.0623 - 541ms/epoch - 14ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 1s - loss: 0.0545 - val_loss: 0.0610 - 507ms/epoch - 13ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0533 - val_loss: 0.0597 - 331ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0522 - val_loss: 0.0584 - 328ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0513 - val_loss: 0.0573 - 312ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0506 - val_loss: 0.0567 - 296ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0509 - val_loss: 0.0570 - 322ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0512 - val_loss: 0.0558 - 331ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0491 - val_loss: 0.0541 - 328ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0475 - val_loss: 0.0531 - 367ms/epoch - 9ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 1s - loss: 0.0468 - val_loss: 0.0521 - 514ms/epoch - 13ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0461 - val_loss: 0.0522 - 331ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0450 - val_loss: 0.0508 - 393ms/epoch - 10ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0445 - val_loss: 0.0517 - 311ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0498 - val_loss: 0.0683 - 340ms/epoch - 9ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0521 - val_loss: 0.0494 - 349ms/epoch - 9ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0420 - val_loss: 0.0482 - 369ms/epoch - 9ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0420 - val_loss: 0.0482 - 349ms/epoch - 9ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0424 - val_loss: 0.0486 - 344ms/epoch - 9ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0420 - val_loss: 0.0469 - 361ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0411 - val_loss: 0.0477 - 328ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0426 - val_loss: 0.0494 - 324ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0434 - val_loss: 0.0492 - 319ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0423 - val_loss: 0.0449 - 324ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0442 - 314ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0449 - 331ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0438 - 301ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 300ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0466 - 338ms/epoch - 9ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0419 - val_loss: 0.0433 - 356ms/epoch - 9ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0432 - 372ms/epoch - 10ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0444 - 382ms/epoch - 10ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 1s - loss: 0.0385 - val_loss: 0.0430 - 684ms/epoch - 18ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0432 - 325ms/epoch - 8ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0432 - 444ms/epoch - 11ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 321ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0431 - 319ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0427 - 344ms/epoch - 9ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0429 - 419ms/epoch - 11ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 1s - loss: 0.0374 - val_loss: 0.0427 - 706ms/epoch - 18ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 1s - loss: 0.0370 - val_loss: 0.0426 - 706ms/epoch - 18ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 1s - loss: 0.0370 - val_loss: 0.0427 - 642ms/epoch - 16ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0425 - 406ms/epoch - 10ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 1s - loss: 0.0369 - val_loss: 0.0426 - 595ms/epoch - 15ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 1s - loss: 0.0370 - val_loss: 0.0426 - 606ms/epoch - 16ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0425 - 405ms/epoch - 10ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0425 - 473ms/epoch - 12ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0425 - 379ms/epoch - 10ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0425 - 329ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0425 - 337ms/epoch - 9ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0425 - 291ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0425 - 326ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0425 - 303ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 316ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 302ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 286ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 296ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 323ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 3, 53)             11660     \n",
      "                                                                 \n",
      " attention_layer_3 (Attentio  (None, 53)               12        \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 54        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,726\n",
      "Trainable params: 11,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 2s - loss: 0.2454 - val_loss: 0.0646 - 2s/epoch - 41ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0561 - val_loss: 0.0616 - 480ms/epoch - 12ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0542 - val_loss: 0.0606 - 318ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0530 - val_loss: 0.0592 - 421ms/epoch - 11ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0520 - val_loss: 0.0580 - 440ms/epoch - 11ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0512 - val_loss: 0.0571 - 415ms/epoch - 11ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 1s - loss: 0.0512 - val_loss: 0.0572 - 555ms/epoch - 14ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0518 - val_loss: 0.0566 - 320ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 1s - loss: 0.0501 - val_loss: 0.0546 - 512ms/epoch - 13ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 1s - loss: 0.0482 - val_loss: 0.0537 - 553ms/epoch - 14ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0477 - val_loss: 0.0533 - 336ms/epoch - 9ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0475 - val_loss: 0.0524 - 354ms/epoch - 9ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0462 - val_loss: 0.0510 - 339ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0449 - val_loss: 0.0498 - 317ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0440 - val_loss: 0.0489 - 405ms/epoch - 10ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 1s - loss: 0.0432 - val_loss: 0.0481 - 722ms/epoch - 19ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0440 - val_loss: 0.0494 - 313ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0464 - val_loss: 0.0500 - 470ms/epoch - 12ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0438 - val_loss: 0.0468 - 298ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0405 - val_loss: 0.0456 - 413ms/epoch - 11ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0399 - val_loss: 0.0452 - 282ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0397 - val_loss: 0.0445 - 367ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0449 - 393ms/epoch - 10ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0407 - val_loss: 0.0448 - 363ms/epoch - 9ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0444 - 346ms/epoch - 9ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0438 - 445ms/epoch - 11ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0437 - 335ms/epoch - 9ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0434 - 384ms/epoch - 10ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0433 - 310ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0433 - 346ms/epoch - 9ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 421ms/epoch - 11ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0430 - 357ms/epoch - 9ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 395ms/epoch - 10ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 340ms/epoch - 9ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0428 - 308ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 1s - loss: 0.0373 - val_loss: 0.0428 - 638ms/epoch - 16ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0427 - 279ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 319ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 395ms/epoch - 10ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0427 - 331ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 344ms/epoch - 9ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 331ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0425 - 327ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0425 - 359ms/epoch - 9ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0425 - 327ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0425 - 430ms/epoch - 11ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0425 - 392ms/epoch - 10ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 330ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0425 - 317ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 323ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 303ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 350ms/epoch - 9ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0424 - 341ms/epoch - 9ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0423 - 429ms/epoch - 11ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0424 - 322ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0423 - 314ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0424 - 387ms/epoch - 10ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 358ms/epoch - 9ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 363ms/epoch - 9ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0423 - 307ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 3, 63)             16380     \n",
      "                                                                 \n",
      " attention_layer_4 (Attentio  (None, 63)               12        \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 64        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,456\n",
      "Trainable params: 16,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.5067 - val_loss: 0.0828 - 1s/epoch - 35ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0657 - val_loss: 0.0648 - 346ms/epoch - 9ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0576 - val_loss: 0.0638 - 365ms/epoch - 9ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0562 - val_loss: 0.0630 - 394ms/epoch - 10ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0553 - val_loss: 0.0622 - 358ms/epoch - 9ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0544 - val_loss: 0.0613 - 337ms/epoch - 9ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0535 - val_loss: 0.0604 - 321ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0527 - val_loss: 0.0596 - 364ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0520 - val_loss: 0.0587 - 345ms/epoch - 9ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0512 - val_loss: 0.0578 - 335ms/epoch - 9ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0504 - val_loss: 0.0569 - 379ms/epoch - 10ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0496 - val_loss: 0.0560 - 346ms/epoch - 9ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0489 - val_loss: 0.0551 - 337ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0484 - val_loss: 0.0546 - 349ms/epoch - 9ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0481 - val_loss: 0.0542 - 342ms/epoch - 9ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0476 - val_loss: 0.0534 - 332ms/epoch - 9ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0466 - val_loss: 0.0525 - 373ms/epoch - 10ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0458 - val_loss: 0.0518 - 354ms/epoch - 9ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0451 - val_loss: 0.0511 - 410ms/epoch - 11ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0443 - val_loss: 0.0502 - 335ms/epoch - 9ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0434 - val_loss: 0.0493 - 346ms/epoch - 9ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0427 - val_loss: 0.0488 - 346ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0420 - val_loss: 0.0481 - 350ms/epoch - 9ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0411 - val_loss: 0.0474 - 336ms/epoch - 9ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0421 - val_loss: 0.0498 - 322ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0486 - val_loss: 0.0510 - 334ms/epoch - 9ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0437 - val_loss: 0.0471 - 326ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0401 - val_loss: 0.0460 - 326ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0461 - 304ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0458 - 295ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0450 - 333ms/epoch - 9ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0453 - 318ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0448 - 299ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0444 - 335ms/epoch - 9ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0449 - 403ms/epoch - 10ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0440 - 352ms/epoch - 9ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0440 - 369ms/epoch - 9ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0441 - 423ms/epoch - 11ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0435 - 311ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0438 - 343ms/epoch - 9ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0435 - 307ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0433 - 333ms/epoch - 9ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0435 - 305ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0431 - 321ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0433 - 313ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0431 - 311ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0431 - 341ms/epoch - 9ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0430 - 329ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0429 - 333ms/epoch - 9ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0430 - 353ms/epoch - 9ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 308ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0429 - 395ms/epoch - 10ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0428 - 354ms/epoch - 9ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0427 - 307ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0428 - 330ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 363ms/epoch - 9ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 335ms/epoch - 9ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 361ms/epoch - 9ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0426 - 359ms/epoch - 9ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0426 - 327ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 3, 56)             12992     \n",
      "                                                                 \n",
      " attention_layer_5 (Attentio  (None, 56)               12        \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 57        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,061\n",
      "Trainable params: 13,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.1951 - val_loss: 0.0618 - 1s/epoch - 35ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0542 - val_loss: 0.0586 - 312ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0542 - val_loss: 0.0621 - 348ms/epoch - 9ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0621 - val_loss: 0.0587 - 287ms/epoch - 7ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0520 - val_loss: 0.0553 - 281ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0516 - val_loss: 0.0558 - 392ms/epoch - 10ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0534 - val_loss: 0.0521 - 299ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0487 - val_loss: 0.0565 - 315ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0593 - val_loss: 0.0513 - 325ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0461 - val_loss: 0.0508 - 319ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0463 - val_loss: 0.0488 - 299ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0436 - val_loss: 0.0466 - 322ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0419 - val_loss: 0.0476 - 343ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0473 - val_loss: 0.0491 - 320ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0478 - val_loss: 0.0507 - 348ms/epoch - 9ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0431 - val_loss: 0.0444 - 319ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0439 - 363ms/epoch - 9ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0439 - 386ms/epoch - 10ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0448 - 352ms/epoch - 9ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0425 - val_loss: 0.0443 - 336ms/epoch - 9ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0434 - 400ms/epoch - 10ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0431 - 337ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0429 - 310ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0431 - 375ms/epoch - 10ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0430 - 312ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0431 - 323ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0431 - 323ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0431 - 334ms/epoch - 9ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 397ms/epoch - 10ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 331ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0429 - 308ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 317ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0429 - 328ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 322ms/epoch - 8ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0430 - 399ms/epoch - 10ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 300ms/epoch - 8ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0431 - 318ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0430 - 325ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0430 - 370ms/epoch - 9ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 318ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0431 - 323ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0430 - 341ms/epoch - 9ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0432 - 323ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 314ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0434 - 366ms/epoch - 9ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0432 - 319ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0431 - 330ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 331ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0431 - 328ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 306ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0431 - 339ms/epoch - 9ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0433 - 330ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0430 - 319ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0431 - 322ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0432 - 293ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0432 - 361ms/epoch - 9ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0431 - 393ms/epoch - 10ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0431 - 394ms/epoch - 10ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0432 - 322ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0431 - 337ms/epoch - 9ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 3, 87)             30972     \n",
      "                                                                 \n",
      " attention_layer_6 (Attentio  (None, 87)               12        \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 88        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,072\n",
      "Trainable params: 31,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2673 - val_loss: 0.0856 - 1s/epoch - 30ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0608 - val_loss: 0.0604 - 335ms/epoch - 9ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0532 - val_loss: 0.0581 - 378ms/epoch - 10ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0519 - val_loss: 0.0574 - 346ms/epoch - 9ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0546 - val_loss: 0.0623 - 311ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0635 - val_loss: 0.0570 - 343ms/epoch - 9ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0599 - val_loss: 0.0801 - 339ms/epoch - 9ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0711 - val_loss: 0.0561 - 342ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0493 - val_loss: 0.0555 - 351ms/epoch - 9ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0508 - val_loss: 0.0556 - 336ms/epoch - 9ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0498 - val_loss: 0.0524 - 344ms/epoch - 9ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0453 - val_loss: 0.0514 - 340ms/epoch - 9ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0606 - val_loss: 0.0575 - 327ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0622 - val_loss: 0.0776 - 325ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0731 - val_loss: 0.0561 - 324ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0463 - val_loss: 0.0491 - 311ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0426 - val_loss: 0.0483 - 367ms/epoch - 9ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0456 - val_loss: 0.0497 - 380ms/epoch - 10ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0428 - val_loss: 0.0462 - 316ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0408 - val_loss: 0.0450 - 328ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0401 - val_loss: 0.0440 - 306ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0437 - 329ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0412 - val_loss: 0.0447 - 356ms/epoch - 9ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0411 - val_loss: 0.0459 - 348ms/epoch - 9ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0415 - val_loss: 0.0435 - 361ms/epoch - 9ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0434 - 361ms/epoch - 9ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0428 - 350ms/epoch - 9ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 331ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0429 - 318ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0437 - 352ms/epoch - 9ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0406 - val_loss: 0.0435 - 300ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0429 - 325ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0429 - 312ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0430 - 338ms/epoch - 9ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0432 - 338ms/epoch - 9ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0434 - 332ms/epoch - 9ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0432 - 322ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0430 - 322ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0428 - 328ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0430 - 300ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0434 - 318ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0435 - 308ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0428 - 315ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0428 - 335ms/epoch - 9ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0429 - 314ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0430 - 357ms/epoch - 9ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0430 - 365ms/epoch - 9ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0428 - 335ms/epoch - 9ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0439 - 328ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0401 - val_loss: 0.0446 - 347ms/epoch - 9ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0428 - val_loss: 0.0486 - 348ms/epoch - 9ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0547 - val_loss: 0.0447 - 390ms/epoch - 10ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.1292 - val_loss: 0.2684 - 347ms/epoch - 9ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.1609 - val_loss: 0.0646 - 324ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0486 - val_loss: 0.0507 - 397ms/epoch - 10ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0437 - val_loss: 0.0483 - 341ms/epoch - 9ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 1s - loss: 0.0420 - val_loss: 0.0467 - 501ms/epoch - 13ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0406 - val_loss: 0.0455 - 313ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0447 - 371ms/epoch - 10ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0442 - 294ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 3, 62)             15872     \n",
      "                                                                 \n",
      " attention_layer_7 (Attentio  (None, 62)               12        \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 63        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,947\n",
      "Trainable params: 15,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.3168 - val_loss: 0.0750 - 1s/epoch - 30ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0670 - val_loss: 0.0621 - 343ms/epoch - 9ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0540 - val_loss: 0.0586 - 351ms/epoch - 9ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0518 - val_loss: 0.0570 - 367ms/epoch - 9ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0519 - val_loss: 0.0588 - 393ms/epoch - 10ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0600 - val_loss: 0.0607 - 373ms/epoch - 10ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0575 - val_loss: 0.0595 - 345ms/epoch - 9ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0633 - val_loss: 0.0585 - 357ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0512 - val_loss: 0.0545 - 310ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0495 - val_loss: 0.0540 - 349ms/epoch - 9ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0492 - val_loss: 0.0523 - 382ms/epoch - 10ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0455 - val_loss: 0.0507 - 371ms/epoch - 10ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0461 - val_loss: 0.0634 - 311ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0786 - val_loss: 0.0680 - 355ms/epoch - 9ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0528 - val_loss: 0.0503 - 353ms/epoch - 9ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0427 - val_loss: 0.0473 - 348ms/epoch - 9ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0410 - val_loss: 0.0462 - 353ms/epoch - 9ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0401 - val_loss: 0.0460 - 371ms/epoch - 10ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0452 - 365ms/epoch - 9ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0484 - 365ms/epoch - 9ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0489 - val_loss: 0.0471 - 340ms/epoch - 9ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0399 - val_loss: 0.0436 - 333ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0438 - 344ms/epoch - 9ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0428 - 345ms/epoch - 9ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0431 - 379ms/epoch - 10ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0419 - val_loss: 0.0445 - 397ms/epoch - 10ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0405 - val_loss: 0.0435 - 359ms/epoch - 9ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0436 - 396ms/epoch - 10ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0431 - 491ms/epoch - 13ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0429 - 394ms/epoch - 10ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0427 - 398ms/epoch - 10ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0427 - 396ms/epoch - 10ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0427 - 345ms/epoch - 9ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0428 - 292ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 324ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0429 - 344ms/epoch - 9ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0429 - 358ms/epoch - 9ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0428 - 365ms/epoch - 9ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0428 - 356ms/epoch - 9ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0429 - 312ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 320ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 331ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0428 - 367ms/epoch - 9ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0429 - 369ms/epoch - 9ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 310ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 354ms/epoch - 9ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0431 - 368ms/epoch - 9ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0431 - 340ms/epoch - 9ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0431 - 425ms/epoch - 11ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 338ms/epoch - 9ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 293ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 283ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 281ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0431 - 292ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0428 - 314ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0427 - 287ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 329ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0432 - 292ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0430 - 310ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0443 - 316ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 3, 60)             14880     \n",
      "                                                                 \n",
      " attention_layer_8 (Attentio  (None, 60)               12        \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,953\n",
      "Trainable params: 14,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2287 - val_loss: 0.0661 - 1s/epoch - 37ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0551 - val_loss: 0.0601 - 393ms/epoch - 10ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0525 - val_loss: 0.0583 - 392ms/epoch - 10ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0511 - val_loss: 0.0569 - 383ms/epoch - 10ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0508 - val_loss: 0.0575 - 446ms/epoch - 11ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0539 - val_loss: 0.0596 - 341ms/epoch - 9ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0528 - val_loss: 0.0545 - 330ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0477 - val_loss: 0.0532 - 304ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0473 - val_loss: 0.0527 - 306ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0473 - val_loss: 0.0524 - 345ms/epoch - 9ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0457 - val_loss: 0.0508 - 337ms/epoch - 9ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0450 - val_loss: 0.0513 - 316ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0520 - val_loss: 0.0664 - 394ms/epoch - 10ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0530 - val_loss: 0.0494 - 300ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0427 - val_loss: 0.0473 - 356ms/epoch - 9ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0410 - val_loss: 0.0466 - 333ms/epoch - 9ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0413 - val_loss: 0.0462 - 355ms/epoch - 9ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0410 - val_loss: 0.0449 - 345ms/epoch - 9ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0412 - val_loss: 0.0484 - 354ms/epoch - 9ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0447 - val_loss: 0.0453 - 330ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0445 - 343ms/epoch - 9ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0444 - 345ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0437 - 345ms/epoch - 9ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0447 - 358ms/epoch - 9ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0435 - 332ms/epoch - 9ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0440 - 332ms/epoch - 9ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0432 - 337ms/epoch - 9ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0436 - 330ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0431 - 327ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0434 - 324ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 316ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 314ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0430 - 330ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0429 - 298ms/epoch - 8ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0429 - 283ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0429 - 287ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 310ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 328ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 299ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0427 - 294ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0427 - 326ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 330ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 322ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0428 - 341ms/epoch - 9ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 295ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 310ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 325ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 311ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 321ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 325ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 314ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 354ms/epoch - 9ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 384ms/epoch - 10ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 339ms/epoch - 9ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 400ms/epoch - 10ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0425 - 320ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 327ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 294ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 309ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 315ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 3, 85)             29580     \n",
      "                                                                 \n",
      " attention_layer_9 (Attentio  (None, 85)               12        \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 86        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,678\n",
      "Trainable params: 29,678\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2455 - val_loss: 0.0683 - 1s/epoch - 31ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0565 - val_loss: 0.0619 - 325ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0544 - val_loss: 0.0609 - 320ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0532 - val_loss: 0.0595 - 329ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0524 - val_loss: 0.0586 - 327ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0528 - val_loss: 0.0597 - 344ms/epoch - 9ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0543 - val_loss: 0.0583 - 340ms/epoch - 9ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0512 - val_loss: 0.0559 - 333ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0493 - val_loss: 0.0551 - 335ms/epoch - 9ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0489 - val_loss: 0.0545 - 326ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0483 - val_loss: 0.0532 - 351ms/epoch - 9ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0470 - val_loss: 0.0520 - 345ms/epoch - 9ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0457 - val_loss: 0.0510 - 334ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0443 - val_loss: 0.0500 - 333ms/epoch - 9ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0431 - val_loss: 0.0491 - 346ms/epoch - 9ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0420 - val_loss: 0.0494 - 343ms/epoch - 9ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0530 - val_loss: 0.0604 - 351ms/epoch - 9ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0522 - val_loss: 0.0536 - 378ms/epoch - 10ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0551 - val_loss: 0.0486 - 350ms/epoch - 9ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0414 - val_loss: 0.0466 - 316ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0401 - val_loss: 0.0460 - 305ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0397 - val_loss: 0.0457 - 315ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0399 - val_loss: 0.0455 - 335ms/epoch - 9ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0397 - val_loss: 0.0448 - 334ms/epoch - 9ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0446 - 358ms/epoch - 9ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0443 - 329ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0443 - 339ms/epoch - 9ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0441 - 307ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0440 - 325ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0438 - 312ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0436 - 307ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0434 - 303ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0433 - 322ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 309ms/epoch - 8ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0432 - 305ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 330ms/epoch - 8ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0431 - 328ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0430 - 334ms/epoch - 9ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0430 - 319ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0429 - 313ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0429 - 346ms/epoch - 9ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0429 - 338ms/epoch - 9ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0429 - 329ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 336ms/epoch - 9ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 379ms/epoch - 10ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 381ms/epoch - 10ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0428 - 308ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0428 - 341ms/epoch - 9ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 341ms/epoch - 9ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 308ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 327ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 286ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 304ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 323ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 315ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0427 - 317ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 334ms/epoch - 9ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 308ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 320ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0425 - 309ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "0 0.04232055468013068 [0.003660506243860121, 60]\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 3, 98)             39200     \n",
      "                                                                 \n",
      " attention_layer_10 (Attenti  (None, 98)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39,311\n",
      "Trainable params: 39,311\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.1840 - val_loss: 0.0665 - 1s/epoch - 34ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0555 - val_loss: 0.0591 - 307ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0562 - val_loss: 0.0704 - 294ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0708 - val_loss: 0.0586 - 299ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0513 - val_loss: 0.0571 - 311ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0537 - val_loss: 0.0594 - 309ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0570 - val_loss: 0.0573 - 350ms/epoch - 9ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0563 - val_loss: 0.0645 - 367ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0584 - val_loss: 0.0552 - 335ms/epoch - 9ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0488 - val_loss: 0.0531 - 332ms/epoch - 9ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0477 - val_loss: 0.0524 - 355ms/epoch - 9ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0458 - val_loss: 0.0498 - 325ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0447 - val_loss: 0.0541 - 333ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0544 - val_loss: 0.0617 - 311ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0615 - val_loss: 0.0510 - 335ms/epoch - 9ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0435 - val_loss: 0.0471 - 358ms/epoch - 9ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0412 - val_loss: 0.0463 - 348ms/epoch - 9ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0412 - val_loss: 0.0456 - 317ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0406 - val_loss: 0.0460 - 326ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0444 - val_loss: 0.0451 - 384ms/epoch - 10ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0403 - val_loss: 0.0455 - 317ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0406 - val_loss: 0.0439 - 335ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0437 - 332ms/epoch - 9ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0434 - 314ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0440 - 336ms/epoch - 9ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0401 - val_loss: 0.0433 - 355ms/epoch - 9ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0437 - 334ms/epoch - 9ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0428 - 317ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0432 - 334ms/epoch - 9ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0428 - 314ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0432 - 310ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0429 - 323ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0432 - 337ms/epoch - 9ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0430 - 322ms/epoch - 8ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 346ms/epoch - 9ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 324ms/epoch - 8ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0431 - 366ms/epoch - 9ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0429 - 333ms/epoch - 9ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 321ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 338ms/epoch - 9ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 338ms/epoch - 9ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 340ms/epoch - 9ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 333ms/epoch - 9ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 325ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0430 - 302ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0429 - 363ms/epoch - 9ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 317ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0430 - 307ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0430 - 333ms/epoch - 9ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0434 - 314ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0432 - 357ms/epoch - 9ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0437 - 362ms/epoch - 9ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0436 - 336ms/epoch - 9ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0434 - 349ms/epoch - 9ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 345ms/epoch - 9ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0431 - 349ms/epoch - 9ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 339ms/epoch - 9ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0428 - 334ms/epoch - 9ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0430 - 326ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0431 - 324ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 3, 37)             5772      \n",
      "                                                                 \n",
      " attention_layer_11 (Attenti  (None, 37)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 38        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,822\n",
      "Trainable params: 5,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2792 - val_loss: 0.0744 - 1s/epoch - 31ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0576 - val_loss: 0.0622 - 306ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0542 - val_loss: 0.0603 - 357ms/epoch - 9ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0527 - val_loss: 0.0592 - 342ms/epoch - 9ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0516 - val_loss: 0.0581 - 331ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0506 - val_loss: 0.0571 - 379ms/epoch - 10ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0497 - val_loss: 0.0561 - 328ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0489 - val_loss: 0.0552 - 338ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0482 - val_loss: 0.0543 - 367ms/epoch - 9ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0477 - val_loss: 0.0537 - 344ms/epoch - 9ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0475 - val_loss: 0.0533 - 322ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0474 - val_loss: 0.0528 - 325ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0466 - val_loss: 0.0517 - 313ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0454 - val_loss: 0.0508 - 346ms/epoch - 9ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0446 - val_loss: 0.0502 - 326ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0439 - val_loss: 0.0495 - 326ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0433 - val_loss: 0.0489 - 341ms/epoch - 9ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0427 - val_loss: 0.0483 - 367ms/epoch - 9ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0422 - val_loss: 0.0476 - 331ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0415 - val_loss: 0.0471 - 342ms/epoch - 9ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0411 - val_loss: 0.0467 - 317ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0408 - val_loss: 0.0461 - 330ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0405 - val_loss: 0.0461 - 320ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0410 - val_loss: 0.0461 - 342ms/epoch - 9ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0408 - val_loss: 0.0457 - 348ms/epoch - 9ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0400 - val_loss: 0.0450 - 412ms/epoch - 11ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0445 - 357ms/epoch - 9ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0442 - 312ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0441 - 322ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0439 - 332ms/epoch - 9ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0437 - 320ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0436 - 326ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0434 - 307ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0433 - 277ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0431 - 313ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0430 - 339ms/epoch - 9ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0429 - 324ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0429 - 284ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 299ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 295ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 318ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 319ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 305ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 314ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 372ms/epoch - 10ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0425 - 330ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0425 - 339ms/epoch - 9ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0425 - 342ms/epoch - 9ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0425 - 319ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0425 - 296ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 297ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 311ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 327ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 307ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 307ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0424 - 309ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0425 - 327ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0425 - 334ms/epoch - 9ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 325ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0425 - 362ms/epoch - 9ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 3, 94)             36096     \n",
      "                                                                 \n",
      " attention_layer_12 (Attenti  (None, 94)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 95        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,203\n",
      "Trainable params: 36,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2437 - val_loss: 0.0683 - 1s/epoch - 32ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0586 - val_loss: 0.0622 - 295ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0582 - val_loss: 0.0709 - 318ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0678 - val_loss: 0.0604 - 360ms/epoch - 9ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0534 - val_loss: 0.0588 - 354ms/epoch - 9ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0547 - val_loss: 0.0625 - 312ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0594 - val_loss: 0.0592 - 326ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0540 - val_loss: 0.0576 - 340ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0531 - val_loss: 0.0564 - 379ms/epoch - 10ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0520 - val_loss: 0.0553 - 340ms/epoch - 9ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0518 - val_loss: 0.0554 - 400ms/epoch - 10ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0532 - val_loss: 0.0574 - 319ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0560 - val_loss: 0.0592 - 333ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0529 - val_loss: 0.0538 - 330ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0472 - val_loss: 0.0505 - 341ms/epoch - 9ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0444 - val_loss: 0.0489 - 328ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0426 - val_loss: 0.0475 - 329ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0409 - val_loss: 0.0465 - 333ms/epoch - 9ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0404 - val_loss: 0.0485 - 323ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0539 - val_loss: 0.0601 - 369ms/epoch - 9ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0502 - val_loss: 0.0493 - 349ms/epoch - 9ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0472 - val_loss: 0.0531 - 346ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0449 - val_loss: 0.0455 - 351ms/epoch - 9ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0443 - 304ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0436 - 345ms/epoch - 9ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0434 - 328ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0397 - val_loss: 0.0438 - 361ms/epoch - 9ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0462 - 337ms/epoch - 9ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0409 - val_loss: 0.0438 - 330ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0434 - 343ms/epoch - 9ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0430 - 345ms/epoch - 9ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0433 - 336ms/epoch - 9ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0433 - 336ms/epoch - 9ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0440 - 368ms/epoch - 9ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0434 - 324ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0432 - 298ms/epoch - 8ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 310ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 317ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 327ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0433 - 309ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0433 - 328ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0432 - 315ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0431 - 315ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 308ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 320ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 315ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0431 - 326ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 331ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 330ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 348ms/epoch - 9ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 362ms/epoch - 9ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 304ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0432 - 308ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0439 - 327ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0440 - 312ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0421 - 325ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0430 - 297ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 321ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 320ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0429 - 345ms/epoch - 9ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 3, 51)             10812     \n",
      "                                                                 \n",
      " attention_layer_13 (Attenti  (None, 51)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 52        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,876\n",
      "Trainable params: 10,876\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.3116 - val_loss: 0.0666 - 1s/epoch - 35ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0580 - val_loss: 0.0624 - 355ms/epoch - 9ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0552 - val_loss: 0.0612 - 349ms/epoch - 9ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0538 - val_loss: 0.0597 - 324ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0525 - val_loss: 0.0583 - 341ms/epoch - 9ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0517 - val_loss: 0.0575 - 383ms/epoch - 10ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0523 - val_loss: 0.0587 - 396ms/epoch - 10ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0539 - val_loss: 0.0570 - 364ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0504 - val_loss: 0.0545 - 365ms/epoch - 9ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0482 - val_loss: 0.0534 - 340ms/epoch - 9ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0476 - val_loss: 0.0525 - 345ms/epoch - 9ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0469 - val_loss: 0.0517 - 371ms/epoch - 10ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0457 - val_loss: 0.0500 - 355ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0441 - val_loss: 0.0505 - 376ms/epoch - 10ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0497 - val_loss: 0.0538 - 344ms/epoch - 9ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0501 - val_loss: 0.0526 - 330ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0454 - val_loss: 0.0479 - 343ms/epoch - 9ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0414 - val_loss: 0.0470 - 343ms/epoch - 9ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0413 - val_loss: 0.0469 - 330ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0420 - val_loss: 0.0464 - 312ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0406 - val_loss: 0.0450 - 314ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0438 - val_loss: 0.0543 - 318ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0458 - val_loss: 0.0447 - 311ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0440 - 329ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0446 - 345ms/epoch - 9ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0404 - val_loss: 0.0453 - 352ms/epoch - 9ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0400 - val_loss: 0.0439 - 465ms/epoch - 12ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0440 - 471ms/epoch - 12ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0439 - 293ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0435 - 327ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0437 - 356ms/epoch - 9ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0434 - 313ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0433 - 307ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0432 - 298ms/epoch - 8ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 328ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 353ms/epoch - 9ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0430 - 301ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 323ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0429 - 350ms/epoch - 9ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 277ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 316ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 314ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 304ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 298ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 463ms/epoch - 12ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 331ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 332ms/epoch - 9ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0425 - 310ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0425 - 359ms/epoch - 9ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0425 - 375ms/epoch - 10ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 1s - loss: 0.0368 - val_loss: 0.0425 - 518ms/epoch - 13ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0424 - 388ms/epoch - 10ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 418ms/epoch - 11ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 377ms/epoch - 10ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 312ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 299ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 350ms/epoch - 9ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0424 - 308ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0423 - 302ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0424 - 296ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 3, 80)             26240     \n",
      "                                                                 \n",
      " attention_layer_14 (Attenti  (None, 80)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,333\n",
      "Trainable params: 26,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2732 - val_loss: 0.0992 - 1s/epoch - 38ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0736 - val_loss: 0.0668 - 324ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0572 - val_loss: 0.0593 - 479ms/epoch - 12ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0550 - val_loss: 0.0653 - 357ms/epoch - 9ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 1s - loss: 0.0694 - val_loss: 0.0607 - 574ms/epoch - 15ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0557 - val_loss: 0.0587 - 389ms/epoch - 10ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0586 - val_loss: 0.0601 - 392ms/epoch - 10ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0602 - val_loss: 0.0688 - 350ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0710 - val_loss: 0.0554 - 413ms/epoch - 11ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0487 - val_loss: 0.0552 - 295ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0513 - val_loss: 0.0554 - 316ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0497 - val_loss: 0.0515 - 318ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0447 - val_loss: 0.0519 - 330ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0592 - val_loss: 0.0495 - 363ms/epoch - 9ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0486 - val_loss: 0.0646 - 376ms/epoch - 10ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0677 - val_loss: 0.0487 - 385ms/epoch - 10ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0424 - val_loss: 0.0464 - 467ms/epoch - 12ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 1s - loss: 0.0429 - val_loss: 0.0513 - 521ms/epoch - 13ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0471 - val_loss: 0.0483 - 335ms/epoch - 9ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0459 - val_loss: 0.0472 - 428ms/epoch - 11ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0407 - val_loss: 0.0443 - 299ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0438 - 366ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0434 - 321ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 341ms/epoch - 9ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0460 - 311ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0426 - val_loss: 0.0445 - 341ms/epoch - 9ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0412 - val_loss: 0.0451 - 356ms/epoch - 9ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0439 - 325ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0429 - 340ms/epoch - 9ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 343ms/epoch - 9ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0428 - 351ms/epoch - 9ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0435 - 331ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0397 - val_loss: 0.0439 - 335ms/epoch - 9ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0437 - 305ms/epoch - 8ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0432 - 305ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0430 - 332ms/epoch - 9ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0431 - 341ms/epoch - 9ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0433 - 334ms/epoch - 9ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 1s - loss: 0.0388 - val_loss: 0.0436 - 518ms/epoch - 13ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0435 - 358ms/epoch - 9ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0438 - 342ms/epoch - 9ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0430 - 336ms/epoch - 9ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0431 - 363ms/epoch - 9ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0432 - 383ms/epoch - 10ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0432 - 322ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0437 - 325ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0441 - 331ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0432 - 308ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0427 - 346ms/epoch - 9ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0434 - 344ms/epoch - 9ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0433 - 349ms/epoch - 9ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0431 - 343ms/epoch - 9ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0438 - 310ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0432 - 320ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0428 - 336ms/epoch - 9ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0427 - 335ms/epoch - 9ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0438 - 330ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0446 - 341ms/epoch - 9ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0416 - val_loss: 0.0481 - 322ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0481 - val_loss: 0.0773 - 302ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "1 0.04232055468013068 [0.0035257701844836156, 51]\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 3, 67)             18492     \n",
      "                                                                 \n",
      " attention_layer_15 (Attenti  (None, 67)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,572\n",
      "Trainable params: 18,572\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.3273 - val_loss: 0.0764 - 1s/epoch - 31ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0619 - val_loss: 0.0628 - 297ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0557 - val_loss: 0.0604 - 432ms/epoch - 11ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0537 - val_loss: 0.0588 - 319ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0532 - val_loss: 0.0591 - 451ms/epoch - 12ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0577 - val_loss: 0.0611 - 363ms/epoch - 9ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0576 - val_loss: 0.0572 - 388ms/epoch - 10ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0562 - val_loss: 0.0635 - 450ms/epoch - 12ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0589 - val_loss: 0.0564 - 400ms/epoch - 10ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 1s - loss: 0.0509 - val_loss: 0.0554 - 502ms/epoch - 13ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0509 - val_loss: 0.0546 - 429ms/epoch - 11ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0491 - val_loss: 0.0520 - 472ms/epoch - 12ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0478 - val_loss: 0.0555 - 386ms/epoch - 10ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0568 - val_loss: 0.0650 - 372ms/epoch - 10ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0620 - val_loss: 0.0534 - 332ms/epoch - 9ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0457 - val_loss: 0.0500 - 305ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0437 - val_loss: 0.0490 - 333ms/epoch - 9ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0433 - val_loss: 0.0492 - 312ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0427 - val_loss: 0.0475 - 334ms/epoch - 9ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0418 - val_loss: 0.0507 - 348ms/epoch - 9ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0552 - val_loss: 0.0482 - 312ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0415 - val_loss: 0.0456 - 281ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0400 - val_loss: 0.0448 - 286ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0442 - 282ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0439 - 285ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0404 - val_loss: 0.0447 - 263ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0407 - val_loss: 0.0447 - 262ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0401 - val_loss: 0.0436 - 274ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0431 - 277ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0430 - 320ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0428 - 299ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0433 - 309ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0432 - 307ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0433 - 289ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0429 - 267ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0429 - 287ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0427 - 315ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 303ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0428 - 331ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0430 - 304ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 294ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0428 - 281ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 281ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0427 - 284ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 290ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0427 - 267ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 264ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 273ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0428 - 291ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 286ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0429 - 272ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0429 - 267ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 262ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 270ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 259ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 266ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 282ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0428 - 267ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 279ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 266ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_16 (LSTM)              (None, 3, 54)             12096     \n",
      "                                                                 \n",
      " attention_layer_16 (Attenti  (None, 54)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 55        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,163\n",
      "Trainable params: 12,163\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.4649 - val_loss: 0.0944 - 1s/epoch - 28ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0626 - val_loss: 0.0648 - 301ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0580 - val_loss: 0.0638 - 289ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0567 - val_loss: 0.0629 - 280ms/epoch - 7ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0559 - val_loss: 0.0621 - 292ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0550 - val_loss: 0.0614 - 289ms/epoch - 7ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0542 - val_loss: 0.0607 - 272ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0534 - val_loss: 0.0601 - 284ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0526 - val_loss: 0.0594 - 282ms/epoch - 7ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0519 - val_loss: 0.0587 - 304ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0512 - val_loss: 0.0580 - 303ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0505 - val_loss: 0.0573 - 287ms/epoch - 7ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0498 - val_loss: 0.0565 - 270ms/epoch - 7ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0491 - val_loss: 0.0556 - 285ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0484 - val_loss: 0.0548 - 297ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0476 - val_loss: 0.0538 - 283ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0467 - val_loss: 0.0529 - 271ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0459 - val_loss: 0.0519 - 289ms/epoch - 7ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0451 - val_loss: 0.0511 - 283ms/epoch - 7ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0444 - val_loss: 0.0504 - 287ms/epoch - 7ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0439 - val_loss: 0.0498 - 279ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0433 - val_loss: 0.0492 - 273ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0426 - val_loss: 0.0485 - 270ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0418 - val_loss: 0.0480 - 279ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0414 - val_loss: 0.0492 - 278ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0422 - val_loss: 0.0502 - 283ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0422 - val_loss: 0.0473 - 284ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0411 - val_loss: 0.0487 - 283ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0434 - val_loss: 0.0479 - 281ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0408 - val_loss: 0.0467 - 282ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0407 - val_loss: 0.0473 - 290ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0413 - val_loss: 0.0465 - 281ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0457 - 279ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0454 - 288ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0452 - 264ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0450 - 265ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0446 - 289ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0445 - 267ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0443 - 265ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0441 - 262ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0439 - 264ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0438 - 301ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0438 - 296ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0437 - 286ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0435 - 267ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0435 - 291ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0434 - 325ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0433 - 324ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0432 - 279ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0432 - 267ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0431 - 261ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0430 - 275ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0430 - 274ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0429 - 282ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0429 - 276ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0429 - 275ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0428 - 296ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0428 - 329ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 305ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0427 - 316ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_17 (LSTM)              (None, 3, 46)             8832      \n",
      "                                                                 \n",
      " attention_layer_17 (Attenti  (None, 46)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 47        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,891\n",
      "Trainable params: 8,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2648 - val_loss: 0.0634 - 1s/epoch - 32ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0606 - val_loss: 0.0606 - 338ms/epoch - 9ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0527 - val_loss: 0.0576 - 256ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0534 - val_loss: 0.0657 - 251ms/epoch - 6ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0705 - val_loss: 0.0565 - 352ms/epoch - 9ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0495 - val_loss: 0.0553 - 312ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0529 - val_loss: 0.0591 - 249ms/epoch - 6ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0576 - val_loss: 0.0528 - 346ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0499 - val_loss: 0.0590 - 320ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0626 - val_loss: 0.0517 - 247ms/epoch - 6ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0458 - val_loss: 0.0506 - 255ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0466 - val_loss: 0.0508 - 314ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0460 - val_loss: 0.0473 - 345ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0414 - val_loss: 0.0469 - 271ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0496 - val_loss: 0.0490 - 298ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0523 - val_loss: 0.0676 - 319ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0502 - val_loss: 0.0449 - 326ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0435 - 322ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0439 - 310ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0436 - 307ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0401 - val_loss: 0.0448 - 437ms/epoch - 11ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0407 - val_loss: 0.0434 - 351ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0430 - 307ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0427 - 323ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0427 - 287ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0428 - 276ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0430 - 280ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0432 - 281ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0428 - 287ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0429 - 293ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0427 - 297ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0428 - 299ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 291ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 310ms/epoch - 8ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 288ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 290ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0429 - 297ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 280ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0431 - 289ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0431 - 290ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0432 - 291ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0430 - 290ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0431 - 293ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0431 - 289ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 292ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 292ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 298ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 288ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0429 - 283ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 273ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 273ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0429 - 277ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 275ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0430 - 266ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 270ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0428 - 287ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0429 - 301ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 303ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0430 - 282ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0431 - 295ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_19 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_18 (LSTM)              (None, 3, 47)             9212      \n",
      "                                                                 \n",
      " attention_layer_18 (Attenti  (None, 47)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 48        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,272\n",
      "Trainable params: 9,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2721 - val_loss: 0.0750 - 1s/epoch - 28ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0622 - val_loss: 0.0600 - 299ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0545 - val_loss: 0.0674 - 298ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0837 - val_loss: 0.0648 - 307ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0545 - val_loss: 0.0562 - 286ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0544 - val_loss: 0.0634 - 304ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0633 - val_loss: 0.0545 - 295ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0530 - val_loss: 0.0647 - 286ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0632 - val_loss: 0.0520 - 289ms/epoch - 7ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0455 - val_loss: 0.0509 - 297ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0475 - val_loss: 0.0548 - 292ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0490 - val_loss: 0.0506 - 293ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0444 - val_loss: 0.0470 - 300ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0409 - val_loss: 0.0461 - 300ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0515 - val_loss: 0.0503 - 289ms/epoch - 7ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0497 - val_loss: 0.0494 - 283ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0427 - val_loss: 0.0438 - 289ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0435 - 296ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0434 - 306ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0423 - 302ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0405 - val_loss: 0.0470 - 303ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0436 - val_loss: 0.0445 - 296ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0399 - val_loss: 0.0434 - 291ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0426 - 291ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0426 - 290ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0426 - 283ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0433 - 296ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0397 - val_loss: 0.0438 - 293ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0442 - 289ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0432 - 292ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0429 - 303ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0429 - 278ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0431 - 269ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0434 - 282ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0436 - 272ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0436 - 274ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0433 - 284ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0429 - 272ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0432 - 272ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0433 - 271ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0436 - 299ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0435 - 288ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0438 - 295ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0399 - val_loss: 0.0438 - 292ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0429 - 298ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0432 - 278ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0433 - 284ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0434 - 298ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0435 - 290ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0431 - 287ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 297ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 279ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 283ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0435 - 294ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0435 - 286ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0431 - 285ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0428 - 290ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0429 - 293ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0431 - 282ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0433 - 288ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_19 (LSTM)              (None, 3, 58)             13920     \n",
      "                                                                 \n",
      " attention_layer_19 (Attenti  (None, 58)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 59        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,991\n",
      "Trainable params: 13,991\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.3084 - val_loss: 0.0727 - 1s/epoch - 28ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0595 - val_loss: 0.0629 - 300ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0557 - val_loss: 0.0613 - 285ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0540 - val_loss: 0.0597 - 311ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0530 - val_loss: 0.0588 - 294ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0536 - val_loss: 0.0607 - 289ms/epoch - 7ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0575 - val_loss: 0.0602 - 284ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0536 - val_loss: 0.0565 - 280ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0506 - val_loss: 0.0553 - 301ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0502 - val_loss: 0.0546 - 287ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0488 - val_loss: 0.0522 - 281ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0452 - val_loss: 0.0510 - 283ms/epoch - 7ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0488 - val_loss: 0.0586 - 288ms/epoch - 7ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0568 - val_loss: 0.0743 - 283ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0743 - val_loss: 0.0587 - 293ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0472 - val_loss: 0.0494 - 294ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0425 - val_loss: 0.0485 - 291ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0418 - val_loss: 0.0475 - 282ms/epoch - 7ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0410 - val_loss: 0.0467 - 292ms/epoch - 7ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0405 - val_loss: 0.0465 - 277ms/epoch - 7ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0400 - val_loss: 0.0462 - 289ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0405 - val_loss: 0.0466 - 291ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0436 - val_loss: 0.0525 - 276ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0444 - val_loss: 0.0447 - 302ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0443 - 287ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0439 - 309ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0435 - 286ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0438 - 291ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0434 - 291ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0434 - 295ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0430 - 286ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 281ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0429 - 273ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 270ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0428 - 265ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0427 - 281ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 270ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0426 - 270ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 276ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0426 - 285ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0426 - 282ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0426 - 278ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 273ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 285ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0427 - 282ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 278ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 284ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 285ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 298ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 280ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 275ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0425 - 277ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0425 - 273ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0425 - 286ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0425 - 290ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 273ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 284ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 284ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 281ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 281ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "2 0.04232055468013068 [0.009785844389116023, 47]\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_21 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_20 (LSTM)              (None, 3, 18)             1440      \n",
      "                                                                 \n",
      " attention_layer_20 (Attenti  (None, 18)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 19        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,471\n",
      "Trainable params: 1,471\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2564 - val_loss: 0.0710 - 1s/epoch - 28ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0568 - val_loss: 0.0622 - 292ms/epoch - 7ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0538 - val_loss: 0.0599 - 303ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0518 - val_loss: 0.0581 - 298ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0500 - val_loss: 0.0563 - 305ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0484 - val_loss: 0.0546 - 295ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0470 - val_loss: 0.0530 - 306ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0458 - val_loss: 0.0515 - 292ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0446 - val_loss: 0.0501 - 289ms/epoch - 7ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0437 - val_loss: 0.0491 - 289ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0430 - val_loss: 0.0483 - 301ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0420 - val_loss: 0.0474 - 290ms/epoch - 7ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0413 - val_loss: 0.0472 - 291ms/epoch - 7ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0416 - val_loss: 0.0478 - 294ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0422 - val_loss: 0.0475 - 289ms/epoch - 7ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0413 - val_loss: 0.0457 - 305ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0450 - 319ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0448 - 294ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0446 - 294ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0445 - 301ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0441 - 316ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0438 - 304ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0437 - 307ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0436 - 292ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0433 - 304ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0431 - 296ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0431 - 307ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0431 - 295ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0430 - 301ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0429 - 314ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 306ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0428 - 288ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0428 - 284ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 289ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0427 - 270ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0427 - 289ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0427 - 288ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 287ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 278ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 294ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 287ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0426 - 281ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0426 - 282ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0425 - 298ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0425 - 272ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 285ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 284ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0424 - 287ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 296ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 289ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 299ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 291ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0425 - 286ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0424 - 282ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0425 - 298ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 289ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 293ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0425 - 292ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0425 - 284ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0425 - 276ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_22 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_21 (LSTM)              (None, 3, 21)             1932      \n",
      "                                                                 \n",
      " attention_layer_21 (Attenti  (None, 21)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,966\n",
      "Trainable params: 1,966\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2974 - val_loss: 0.0675 - 1s/epoch - 32ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0580 - val_loss: 0.0604 - 289ms/epoch - 7ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0531 - val_loss: 0.0576 - 290ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0506 - val_loss: 0.0557 - 294ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0524 - val_loss: 0.0615 - 317ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0570 - val_loss: 0.0526 - 300ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0460 - val_loss: 0.0510 - 289ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0465 - val_loss: 0.0522 - 291ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0486 - val_loss: 0.0545 - 295ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0471 - val_loss: 0.0475 - 298ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0416 - val_loss: 0.0488 - 298ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0482 - val_loss: 0.0502 - 303ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0442 - val_loss: 0.0460 - 297ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0405 - val_loss: 0.0458 - 292ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0407 - val_loss: 0.0451 - 300ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0400 - val_loss: 0.0443 - 292ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0399 - val_loss: 0.0453 - 291ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0410 - val_loss: 0.0445 - 302ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0437 - 295ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0438 - 295ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0435 - 291ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0432 - 294ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0431 - 297ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0432 - 291ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 310ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0430 - 287ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 316ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 305ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 296ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0429 - 285ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0426 - 293ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0427 - 268ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 264ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 275ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 280ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 270ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 270ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0429 - 282ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0427 - 272ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 268ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0429 - 272ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0429 - 275ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0428 - 288ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0427 - 276ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 284ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 290ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 280ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0427 - 276ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 304ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0428 - 282ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 283ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0428 - 280ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0427 - 282ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 282ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 283ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 293ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 282ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 280ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0427 - 281ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 268ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_23 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_22 (LSTM)              (None, 3, 62)             15872     \n",
      "                                                                 \n",
      " attention_layer_22 (Attenti  (None, 62)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1)                 63        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,947\n",
      "Trainable params: 15,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2271 - val_loss: 0.0636 - 1s/epoch - 28ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0571 - val_loss: 0.0605 - 327ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0535 - val_loss: 0.0584 - 293ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0551 - val_loss: 0.0651 - 287ms/epoch - 7ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0637 - val_loss: 0.0564 - 290ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0497 - val_loss: 0.0544 - 290ms/epoch - 7ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0504 - val_loss: 0.0596 - 284ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0566 - val_loss: 0.0531 - 295ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0471 - val_loss: 0.0563 - 288ms/epoch - 7ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0637 - val_loss: 0.0521 - 286ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0468 - val_loss: 0.0513 - 286ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0457 - val_loss: 0.0497 - 294ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0446 - val_loss: 0.0537 - 281ms/epoch - 7ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0475 - val_loss: 0.0505 - 286ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0438 - val_loss: 0.0473 - 283ms/epoch - 7ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0465 - val_loss: 0.0543 - 287ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0497 - val_loss: 0.0513 - 292ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0448 - val_loss: 0.0456 - 285ms/epoch - 7ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0444 - 285ms/epoch - 7ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0448 - 288ms/epoch - 7ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0440 - 303ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0418 - val_loss: 0.0465 - 281ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0412 - val_loss: 0.0442 - 291ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0434 - 295ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0430 - 285ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 294ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0431 - 288ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0433 - 289ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0429 - 292ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0429 - 294ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0427 - 290ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 285ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0429 - 273ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0428 - 272ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0430 - 272ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0430 - 286ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0429 - 273ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0427 - 274ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0427 - 279ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 277ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 275ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0428 - 292ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0429 - 277ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0427 - 270ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0427 - 270ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0427 - 267ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 279ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 281ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0430 - 288ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 278ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 287ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0434 - 277ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0430 - 283ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0430 - 280ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0427 - 291ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 288ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 280ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 278ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0427 - 281ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0431 - 271ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_24 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_23 (LSTM)              (None, 3, 23)             2300      \n",
      "                                                                 \n",
      " attention_layer_23 (Attenti  (None, 23)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 24        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,336\n",
      "Trainable params: 2,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.3821 - val_loss: 0.0685 - 1s/epoch - 28ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0610 - val_loss: 0.0614 - 294ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0551 - val_loss: 0.0587 - 313ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0537 - val_loss: 0.0599 - 300ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0624 - val_loss: 0.0619 - 296ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0533 - val_loss: 0.0532 - 285ms/epoch - 7ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0482 - val_loss: 0.0535 - 284ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0585 - val_loss: 0.0556 - 290ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0549 - val_loss: 0.0518 - 299ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0523 - val_loss: 0.0559 - 294ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0486 - val_loss: 0.0481 - 289ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0424 - val_loss: 0.0471 - 295ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0434 - val_loss: 0.0503 - 299ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0442 - val_loss: 0.0471 - 281ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0422 - val_loss: 0.0481 - 287ms/epoch - 7ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0435 - val_loss: 0.0475 - 306ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0446 - val_loss: 0.0490 - 290ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0416 - val_loss: 0.0440 - 287ms/epoch - 7ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 282ms/epoch - 7ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0403 - val_loss: 0.0502 - 296ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0455 - val_loss: 0.0433 - 296ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 290ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0435 - 293ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0433 - 291ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0428 - 303ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0432 - 297ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0434 - 297ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0431 - 310ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 307ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0432 - 303ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0428 - 302ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 301ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0433 - 278ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0432 - 278ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 283ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 273ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0430 - 282ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0432 - 295ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0432 - 292ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 277ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0433 - 272ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0432 - 281ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0431 - 296ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 315ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0433 - 288ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0433 - 283ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0434 - 287ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0434 - 280ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0434 - 280ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0433 - 273ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0433 - 281ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0433 - 278ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0433 - 279ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0434 - 287ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0434 - 283ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0433 - 311ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0434 - 292ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0434 - 282ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0433 - 300ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0433 - 291ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_25 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_24 (LSTM)              (None, 3, 58)             13920     \n",
      "                                                                 \n",
      " attention_layer_24 (Attenti  (None, 58)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 1)                 59        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,991\n",
      "Trainable params: 13,991\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2474 - val_loss: 0.0707 - 1s/epoch - 28ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0953 - val_loss: 0.0724 - 292ms/epoch - 7ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0584 - val_loss: 0.0581 - 302ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0525 - val_loss: 0.0599 - 292ms/epoch - 7ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0649 - val_loss: 0.0627 - 294ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0612 - val_loss: 0.0644 - 308ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0696 - val_loss: 0.0566 - 287ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0489 - val_loss: 0.0554 - 287ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0522 - val_loss: 0.0552 - 288ms/epoch - 7ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0507 - val_loss: 0.0547 - 286ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0502 - val_loss: 0.0550 - 300ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0496 - val_loss: 0.0556 - 289ms/epoch - 7ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0537 - val_loss: 0.0497 - 291ms/epoch - 7ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0458 - val_loss: 0.0506 - 279ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0463 - val_loss: 0.0461 - 294ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0407 - val_loss: 0.0449 - 285ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0425 - val_loss: 0.0460 - 299ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0414 - val_loss: 0.0447 - 289ms/epoch - 7ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0438 - val_loss: 0.0448 - 296ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0436 - 297ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0432 - 286ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0428 - 300ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 305ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0430 - 298ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0443 - 285ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0406 - val_loss: 0.0440 - 282ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0431 - 292ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0428 - 300ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0428 - 301ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0432 - 298ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0436 - 289ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0437 - 279ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0435 - 269ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0431 - 271ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0432 - 280ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0433 - 270ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0435 - 289ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0435 - 289ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0434 - 301ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0433 - 294ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0433 - 292ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0434 - 287ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0435 - 285ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0444 - 284ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0429 - val_loss: 0.0551 - 279ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0719 - val_loss: 0.1145 - 296ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.1636 - val_loss: 0.3870 - 297ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.1489 - val_loss: 0.0552 - 302ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0468 - val_loss: 0.0490 - 291ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0425 - val_loss: 0.0470 - 294ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0406 - val_loss: 0.0456 - 296ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0454 - 279ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0443 - 276ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0431 - 276ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0428 - 279ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0432 - 272ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0432 - 280ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0425 - 286ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0424 - 275ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 281ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "3 0.04232055468013068 [0.009685864424230067, 23]\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_26 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_25 (LSTM)              (None, 3, 31)             4092      \n",
      "                                                                 \n",
      " attention_layer_25 (Attenti  (None, 31)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1)                 32        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,136\n",
      "Trainable params: 4,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2877 - val_loss: 0.0677 - 1s/epoch - 31ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0566 - val_loss: 0.0603 - 303ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0530 - val_loss: 0.0575 - 311ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0514 - val_loss: 0.0575 - 296ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0556 - val_loss: 0.0592 - 297ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0514 - val_loss: 0.0533 - 285ms/epoch - 7ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0472 - val_loss: 0.0529 - 286ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0486 - val_loss: 0.0537 - 299ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0495 - val_loss: 0.0504 - 304ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0440 - val_loss: 0.0481 - 306ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0436 - val_loss: 0.0568 - 304ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0585 - val_loss: 0.0482 - 295ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0414 - val_loss: 0.0461 - 288ms/epoch - 7ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0407 - val_loss: 0.0462 - 291ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0411 - val_loss: 0.0453 - 308ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0400 - val_loss: 0.0441 - 292ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0460 - 296ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0436 - val_loss: 0.0449 - 284ms/epoch - 7ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0436 - 279ms/epoch - 7ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0443 - 288ms/epoch - 7ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0432 - 274ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0435 - 280ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0435 - 272ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0429 - 271ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0435 - 276ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0431 - 272ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0427 - 273ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0430 - 279ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 269ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 276ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 272ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0426 - 278ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 269ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 258ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 267ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 264ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 268ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0429 - 271ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0426 - 278ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0427 - 273ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0429 - 280ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0426 - 265ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 275ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 260ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0427 - 271ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 269ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0430 - 265ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 265ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0431 - 270ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0427 - 269ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 266ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 265ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 268ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 271ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 263ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 271ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 269ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 267ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0426 - 264ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0425 - 283ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_27 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_26 (LSTM)              (None, 3, 31)             4092      \n",
      "                                                                 \n",
      " attention_layer_26 (Attenti  (None, 31)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 1)                 32        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,136\n",
      "Trainable params: 4,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.3943 - val_loss: 0.0816 - 1s/epoch - 27ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0637 - val_loss: 0.0653 - 284ms/epoch - 7ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0578 - val_loss: 0.0632 - 291ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0559 - val_loss: 0.0617 - 278ms/epoch - 7ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0542 - val_loss: 0.0602 - 283ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0528 - val_loss: 0.0589 - 282ms/epoch - 7ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0516 - val_loss: 0.0576 - 278ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0504 - val_loss: 0.0563 - 289ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0492 - val_loss: 0.0550 - 283ms/epoch - 7ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0480 - val_loss: 0.0536 - 306ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0470 - val_loss: 0.0525 - 291ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0464 - val_loss: 0.0515 - 293ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0455 - val_loss: 0.0500 - 276ms/epoch - 7ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0444 - val_loss: 0.0509 - 287ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0438 - val_loss: 0.0490 - 285ms/epoch - 7ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0421 - val_loss: 0.0485 - 282ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0459 - val_loss: 0.0554 - 275ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0461 - val_loss: 0.0466 - 279ms/epoch - 7ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0458 - 276ms/epoch - 7ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0458 - 282ms/epoch - 7ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0455 - 278ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0450 - 284ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0440 - 275ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0456 - 276ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0449 - val_loss: 0.0474 - 277ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0408 - val_loss: 0.0441 - 283ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0436 - 277ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0438 - 278ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0442 - 280ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0438 - 282ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0434 - 293ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0435 - 281ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0435 - 276ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0433 - 266ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0433 - 280ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0432 - 291ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0432 - 278ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0431 - 261ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0431 - 269ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0430 - 269ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0430 - 271ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0430 - 268ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0430 - 276ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0430 - 265ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0430 - 275ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0430 - 262ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0430 - 266ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0429 - 276ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0429 - 262ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0430 - 288ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0429 - 271ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0429 - 269ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0429 - 264ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0429 - 267ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0429 - 267ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0429 - 258ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0429 - 273ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0429 - 270ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0428 - 268ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0430 - 264ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_28 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_27 (LSTM)              (None, 3, 46)             8832      \n",
      "                                                                 \n",
      " attention_layer_27 (Attenti  (None, 46)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 47        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,891\n",
      "Trainable params: 8,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.6170 - val_loss: 0.1076 - 1s/epoch - 29ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0660 - val_loss: 0.0662 - 303ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0589 - val_loss: 0.0651 - 291ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0577 - val_loss: 0.0640 - 293ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0567 - val_loss: 0.0631 - 286ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0556 - val_loss: 0.0622 - 309ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0547 - val_loss: 0.0614 - 288ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0538 - val_loss: 0.0606 - 282ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0530 - val_loss: 0.0599 - 295ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0522 - val_loss: 0.0592 - 285ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0516 - val_loss: 0.0585 - 282ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0509 - val_loss: 0.0579 - 289ms/epoch - 7ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0503 - val_loss: 0.0572 - 294ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0497 - val_loss: 0.0566 - 280ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0491 - val_loss: 0.0559 - 283ms/epoch - 7ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0485 - val_loss: 0.0552 - 287ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0478 - val_loss: 0.0544 - 292ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0472 - val_loss: 0.0536 - 306ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0465 - val_loss: 0.0528 - 296ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0458 - val_loss: 0.0519 - 295ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0454 - val_loss: 0.0514 - 293ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0454 - val_loss: 0.0511 - 289ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0445 - val_loss: 0.0496 - 290ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0429 - val_loss: 0.0487 - 292ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0426 - val_loss: 0.0486 - 302ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0425 - val_loss: 0.0479 - 294ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0411 - val_loss: 0.0468 - 290ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0403 - val_loss: 0.0465 - 295ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0404 - val_loss: 0.0464 - 314ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0457 - 313ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0455 - 292ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0454 - 282ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0450 - 281ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0449 - 281ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0448 - 271ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0445 - 274ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0444 - 286ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0445 - 276ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0442 - 272ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0441 - 290ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0441 - 283ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0438 - 288ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0437 - 285ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0437 - 274ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0436 - 283ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0435 - 292ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0434 - 286ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0433 - 280ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0432 - 277ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0432 - 290ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0431 - 282ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0431 - 285ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0431 - 280ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0430 - 284ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0430 - 284ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0430 - 289ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0429 - 282ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0429 - 282ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0428 - 282ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0428 - 273ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_29 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_28 (LSTM)              (None, 3, 17)             1292      \n",
      "                                                                 \n",
      " attention_layer_28 (Attenti  (None, 17)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 1)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,322\n",
      "Trainable params: 1,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.4131 - val_loss: 0.0686 - 1s/epoch - 28ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0593 - val_loss: 0.0646 - 278ms/epoch - 7ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0563 - val_loss: 0.0625 - 289ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0542 - val_loss: 0.0605 - 293ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0523 - val_loss: 0.0584 - 280ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0505 - val_loss: 0.0563 - 278ms/epoch - 7ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0488 - val_loss: 0.0546 - 282ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0478 - val_loss: 0.0539 - 316ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0479 - val_loss: 0.0532 - 291ms/epoch - 7ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0467 - val_loss: 0.0511 - 279ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0445 - val_loss: 0.0497 - 309ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0432 - val_loss: 0.0488 - 289ms/epoch - 7ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0425 - val_loss: 0.0500 - 289ms/epoch - 7ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0437 - val_loss: 0.0528 - 291ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0455 - val_loss: 0.0480 - 281ms/epoch - 7ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0416 - val_loss: 0.0486 - 291ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0439 - val_loss: 0.0476 - 306ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0406 - val_loss: 0.0455 - 295ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0450 - 289ms/epoch - 7ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0444 - 287ms/epoch - 7ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0442 - 288ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0474 - 283ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0433 - val_loss: 0.0449 - 284ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0406 - val_loss: 0.0459 - 294ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0435 - 279ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0432 - 288ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0434 - 294ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0432 - 296ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0431 - 293ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0430 - 295ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0429 - 296ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 283ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 303ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 268ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 276ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0425 - 281ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0425 - 280ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0425 - 287ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 271ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0425 - 282ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0425 - 272ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 279ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 274ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0425 - 278ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 266ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0425 - 276ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0425 - 277ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 280ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0425 - 288ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 288ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0425 - 278ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0425 - 282ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 289ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0424 - 282ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0424 - 292ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 304ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 292ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0423 - 301ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 304ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0423 - 287ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_30 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_29 (LSTM)              (None, 3, 38)             6080      \n",
      "                                                                 \n",
      " attention_layer_29 (Attenti  (None, 38)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 39        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,131\n",
      "Trainable params: 6,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2720 - val_loss: 0.0639 - 1s/epoch - 29ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0562 - val_loss: 0.0614 - 298ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0538 - val_loss: 0.0601 - 297ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0524 - val_loss: 0.0585 - 293ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0510 - val_loss: 0.0570 - 295ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0500 - val_loss: 0.0558 - 283ms/epoch - 7ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0492 - val_loss: 0.0550 - 286ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0494 - val_loss: 0.0553 - 282ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0499 - val_loss: 0.0541 - 298ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0478 - val_loss: 0.0522 - 286ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0459 - val_loss: 0.0510 - 283ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0450 - val_loss: 0.0500 - 283ms/epoch - 7ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0443 - val_loss: 0.0490 - 283ms/epoch - 7ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0430 - val_loss: 0.0479 - 286ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0410 - val_loss: 0.0466 - 290ms/epoch - 7ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0476 - 291ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0523 - val_loss: 0.0482 - 287ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0457 - val_loss: 0.0493 - 283ms/epoch - 7ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0424 - val_loss: 0.0457 - 280ms/epoch - 7ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0447 - 283ms/epoch - 7ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0450 - 281ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0446 - 277ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0443 - 286ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0442 - 277ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0437 - 284ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0437 - 287ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0434 - 293ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0434 - 287ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 284ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 284ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0430 - 286ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 276ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 272ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 269ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 285ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 265ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 268ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0425 - 264ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0425 - 269ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0425 - 268ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0424 - 267ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0425 - 274ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 272ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 269ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 271ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 269ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 277ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 266ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 270ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 269ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 262ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 277ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 291ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 261ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 269ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 268ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0423 - 264ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0423 - 264ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0423 - 268ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0424 - 264ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "4 0.04232055468013068 [0.007645415977386515, 17]\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_31 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_30 (LSTM)              (None, 3, 49)             9996      \n",
      "                                                                 \n",
      " attention_layer_30 (Attenti  (None, 49)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 1)                 50        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,058\n",
      "Trainable params: 10,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.1991 - val_loss: 0.0722 - 1s/epoch - 32ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0568 - val_loss: 0.0593 - 302ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0532 - val_loss: 0.0576 - 283ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0553 - val_loss: 0.0656 - 283ms/epoch - 7ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0713 - val_loss: 0.0554 - 281ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0518 - val_loss: 0.0553 - 283ms/epoch - 7ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0574 - val_loss: 0.0627 - 285ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0593 - val_loss: 0.0558 - 280ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0530 - val_loss: 0.0526 - 288ms/epoch - 7ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0481 - val_loss: 0.0494 - 285ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0445 - val_loss: 0.0478 - 287ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0433 - val_loss: 0.0478 - 287ms/epoch - 7ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0431 - val_loss: 0.0452 - 287ms/epoch - 7ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0450 - val_loss: 0.0613 - 286ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0718 - val_loss: 0.0474 - 293ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0420 - val_loss: 0.0447 - 304ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0441 - 301ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0434 - 271ms/epoch - 7ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0442 - 277ms/epoch - 7ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0423 - val_loss: 0.0498 - 277ms/epoch - 7ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0423 - val_loss: 0.0434 - 281ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0427 - 275ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0431 - 277ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0426 - 281ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0436 - 280ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0430 - 273ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0431 - 281ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0428 - 274ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 270ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0431 - 296ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0432 - 315ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0433 - 284ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0434 - 277ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0431 - 266ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0432 - 274ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0431 - 267ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0434 - 275ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0434 - 290ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0434 - 266ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0433 - 267ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0432 - 268ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0435 - 274ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0435 - 258ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0434 - 264ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0433 - 272ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0433 - 260ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0433 - 265ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 272ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0434 - 271ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0437 - 265ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0431 - 285ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0433 - 278ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0432 - 263ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0434 - 264ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0438 - 263ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0430 - 261ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0431 - 283ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0431 - 267ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0432 - 274ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0432 - 262ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_32 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_31 (LSTM)              (None, 3, 19)             1596      \n",
      "                                                                 \n",
      " attention_layer_31 (Attenti  (None, 19)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 1)                 20        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,628\n",
      "Trainable params: 1,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.4863 - val_loss: 0.0924 - 1s/epoch - 30ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0692 - val_loss: 0.0655 - 328ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0582 - val_loss: 0.0633 - 318ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0564 - val_loss: 0.0619 - 318ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0548 - val_loss: 0.0608 - 307ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0536 - val_loss: 0.0598 - 327ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0525 - val_loss: 0.0589 - 320ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0516 - val_loss: 0.0581 - 308ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0507 - val_loss: 0.0573 - 307ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0499 - val_loss: 0.0566 - 303ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0491 - val_loss: 0.0558 - 300ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0483 - val_loss: 0.0551 - 308ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0476 - val_loss: 0.0543 - 325ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0469 - val_loss: 0.0536 - 314ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0462 - val_loss: 0.0528 - 314ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0455 - val_loss: 0.0520 - 319ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0448 - val_loss: 0.0512 - 307ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0441 - val_loss: 0.0505 - 315ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0434 - val_loss: 0.0497 - 302ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0427 - val_loss: 0.0489 - 300ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0421 - val_loss: 0.0482 - 318ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0416 - val_loss: 0.0476 - 316ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0414 - val_loss: 0.0475 - 306ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0414 - val_loss: 0.0469 - 320ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0403 - val_loss: 0.0461 - 314ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0457 - 314ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0454 - 353ms/epoch - 9ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0452 - 311ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0449 - 318ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0445 - 305ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0443 - 312ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0442 - 294ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0440 - 316ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0439 - 293ms/epoch - 8ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0437 - 280ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0436 - 290ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0435 - 301ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0434 - 290ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0433 - 286ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0432 - 288ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0432 - 283ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0431 - 288ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0431 - 311ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0430 - 306ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0430 - 294ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0429 - 302ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0429 - 311ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0428 - 294ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0428 - 289ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0428 - 290ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0427 - 334ms/epoch - 9ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0427 - 282ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0427 - 297ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0426 - 309ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0426 - 303ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0361 - val_loss: 0.0426 - 333ms/epoch - 9ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0361 - val_loss: 0.0425 - 306ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0361 - val_loss: 0.0425 - 305ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0360 - val_loss: 0.0425 - 288ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0360 - val_loss: 0.0424 - 284ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_33 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_32 (LSTM)              (None, 3, 43)             7740      \n",
      "                                                                 \n",
      " attention_layer_32 (Attenti  (None, 43)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 1)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,796\n",
      "Trainable params: 7,796\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2346 - val_loss: 0.0683 - 1s/epoch - 29ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0556 - val_loss: 0.0587 - 300ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0557 - val_loss: 0.0694 - 307ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0663 - val_loss: 0.0564 - 300ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0490 - val_loss: 0.0543 - 289ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0501 - val_loss: 0.0560 - 294ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0537 - val_loss: 0.0566 - 305ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0508 - val_loss: 0.0510 - 304ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0445 - val_loss: 0.0513 - 299ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0544 - val_loss: 0.0525 - 311ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0514 - val_loss: 0.0581 - 292ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0519 - val_loss: 0.0479 - 297ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0411 - val_loss: 0.0466 - 302ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0409 - val_loss: 0.0457 - 318ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0451 - 313ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0438 - 300ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0459 - 331ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0488 - val_loss: 0.0438 - 303ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0438 - 313ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0437 - 305ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0429 - 297ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0437 - 312ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0429 - 310ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0433 - 293ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0428 - 306ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0431 - 295ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0430 - 299ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0431 - 302ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0429 - 321ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 299ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 301ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0431 - 286ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 306ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0432 - 282ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0429 - 307ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0430 - 283ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0430 - 284ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 295ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0432 - 294ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0434 - 297ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0433 - 293ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0434 - 292ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0435 - 302ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0432 - 291ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0428 - 297ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0430 - 281ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0427 - 312ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0429 - 295ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 294ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0433 - 289ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0433 - 289ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0432 - 316ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 306ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0430 - 296ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0430 - 312ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0431 - 296ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0431 - 306ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0429 - 284ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0429 - 281ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0434 - 279ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_34 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_33 (LSTM)              (None, 3, 88)             31680     \n",
      "                                                                 \n",
      " attention_layer_33 (Attenti  (None, 88)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 1)                 89        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,781\n",
      "Trainable params: 31,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.4620 - val_loss: 0.1095 - 1s/epoch - 28ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0827 - val_loss: 0.0621 - 301ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0554 - val_loss: 0.0590 - 310ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0523 - val_loss: 0.0571 - 295ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0526 - val_loss: 0.0597 - 289ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0575 - val_loss: 0.0572 - 310ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0524 - val_loss: 0.0611 - 283ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0541 - val_loss: 0.0626 - 305ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0533 - val_loss: 0.0598 - 297ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0594 - val_loss: 0.0717 - 324ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0540 - val_loss: 0.0553 - 306ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0466 - val_loss: 0.0546 - 339ms/epoch - 9ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0450 - val_loss: 0.0497 - 298ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0471 - val_loss: 0.0658 - 298ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0644 - val_loss: 0.0707 - 305ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0548 - val_loss: 0.0480 - 290ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0422 - val_loss: 0.0458 - 289ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0407 - val_loss: 0.0463 - 300ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0403 - val_loss: 0.0453 - 312ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0415 - val_loss: 0.0500 - 307ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0509 - val_loss: 0.0467 - 347ms/epoch - 9ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0421 - val_loss: 0.0441 - 301ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0440 - 315ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0435 - 300ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0464 - 318ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0482 - val_loss: 0.0495 - 320ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0424 - val_loss: 0.0450 - 299ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0423 - 301ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0447 - 301ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0400 - val_loss: 0.0453 - 311ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0411 - val_loss: 0.0450 - 287ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0424 - val_loss: 0.0440 - 287ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0423 - 301ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0422 - 292ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0437 - 280ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0409 - val_loss: 0.0424 - 289ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0397 - val_loss: 0.0465 - 289ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0489 - val_loss: 0.0478 - 298ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0506 - val_loss: 0.0539 - 302ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0511 - val_loss: 0.0613 - 305ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.1289 - val_loss: 0.1735 - 302ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.1482 - val_loss: 0.0811 - 278ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0626 - val_loss: 0.0534 - 281ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0462 - val_loss: 0.0510 - 286ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0444 - val_loss: 0.0492 - 293ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0428 - val_loss: 0.0475 - 290ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0414 - val_loss: 0.0461 - 306ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0408 - val_loss: 0.0457 - 304ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0408 - val_loss: 0.0446 - 297ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0438 - 309ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0445 - 300ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0415 - val_loss: 0.0495 - 306ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0421 - val_loss: 0.0429 - 286ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0460 - 286ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0424 - val_loss: 0.0435 - 282ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0431 - 283ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0429 - 289ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0433 - 280ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0429 - 277ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 283ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_35 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_34 (LSTM)              (None, 3, 70)             20160     \n",
      "                                                                 \n",
      " attention_layer_34 (Attenti  (None, 70)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 1)                 71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,243\n",
      "Trainable params: 20,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2306 - val_loss: 0.0685 - 1s/epoch - 32ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0604 - val_loss: 0.0619 - 306ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0537 - val_loss: 0.0590 - 310ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0526 - val_loss: 0.0584 - 306ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0545 - val_loss: 0.0623 - 304ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0589 - val_loss: 0.0582 - 299ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0526 - val_loss: 0.0572 - 299ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0527 - val_loss: 0.0564 - 306ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0517 - val_loss: 0.0552 - 297ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0514 - val_loss: 0.0557 - 298ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0525 - val_loss: 0.0558 - 294ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0514 - val_loss: 0.0541 - 301ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0486 - val_loss: 0.0517 - 293ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0462 - val_loss: 0.0502 - 316ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0451 - val_loss: 0.0492 - 294ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0452 - val_loss: 0.0496 - 293ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0477 - val_loss: 0.0523 - 332ms/epoch - 9ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0478 - val_loss: 0.0490 - 297ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0424 - val_loss: 0.0456 - 302ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0400 - val_loss: 0.0446 - 294ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0440 - 300ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0408 - val_loss: 0.0453 - 293ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0423 - val_loss: 0.0470 - 291ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0420 - val_loss: 0.0441 - 304ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0434 - 296ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 322ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0431 - 278ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0430 - 298ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0432 - 297ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0428 - 295ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0429 - 299ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0426 - 285ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0429 - 273ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0426 - 275ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 297ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0426 - 283ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 271ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0427 - 273ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0428 - 281ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0427 - 273ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0429 - 272ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0427 - 350ms/epoch - 9ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 373ms/epoch - 10ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 286ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 307ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 294ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 278ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 285ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 375ms/epoch - 10ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0429 - 276ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 280ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0429 - 297ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0426 - 291ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0425 - 301ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 271ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0425 - 281ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 269ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 291ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0427 - 288ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 288ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "5 0.04232055468013068 [0.009170364440499044, 88]\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_36 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_35 (LSTM)              (None, 3, 81)             26892     \n",
      "                                                                 \n",
      " attention_layer_35 (Attenti  (None, 81)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 82        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,986\n",
      "Trainable params: 26,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2451 - val_loss: 0.0762 - 1s/epoch - 29ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0587 - val_loss: 0.0601 - 296ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0528 - val_loss: 0.0578 - 299ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0515 - val_loss: 0.0570 - 289ms/epoch - 7ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0527 - val_loss: 0.0588 - 303ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0596 - val_loss: 0.0574 - 309ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0541 - val_loss: 0.0558 - 305ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0490 - val_loss: 0.0544 - 307ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0679 - val_loss: 0.0667 - 303ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0742 - val_loss: 0.0887 - 295ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0593 - val_loss: 0.0528 - 297ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0458 - val_loss: 0.0509 - 285ms/epoch - 7ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0449 - val_loss: 0.0503 - 298ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0458 - val_loss: 0.0492 - 286ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0444 - val_loss: 0.0475 - 322ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0461 - val_loss: 0.0504 - 305ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0497 - val_loss: 0.0558 - 301ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0477 - val_loss: 0.0457 - 300ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0399 - val_loss: 0.0442 - 295ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0444 - 298ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0430 - 307ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0454 - 304ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0447 - val_loss: 0.0437 - 288ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0438 - 300ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0425 - val_loss: 0.0441 - 294ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0441 - 307ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0427 - 316ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0427 - 326ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0426 - 314ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0430 - 304ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0437 - 293ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0440 - 316ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0431 - 305ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0427 - 271ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0426 - 293ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0429 - 274ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0438 - 283ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0439 - 281ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0434 - 295ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0427 - 285ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0427 - 295ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0429 - 306ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0435 - 291ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0434 - 299ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0431 - 297ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0428 - 296ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0431 - 293ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0433 - 296ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0432 - 289ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0431 - 276ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 316ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 279ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0432 - 300ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0433 - 293ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0429 - 273ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 308ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 279ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0432 - 304ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0430 - 296ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0431 - 319ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_37 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_36 (LSTM)              (None, 3, 43)             7740      \n",
      "                                                                 \n",
      " attention_layer_36 (Attenti  (None, 43)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 1)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,796\n",
      "Trainable params: 7,796\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.1732 - val_loss: 0.0631 - 1s/epoch - 30ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0548 - val_loss: 0.0594 - 288ms/epoch - 7ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0567 - val_loss: 0.0690 - 280ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0627 - val_loss: 0.0555 - 291ms/epoch - 7ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0488 - val_loss: 0.0546 - 304ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0517 - val_loss: 0.0568 - 292ms/epoch - 7ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0534 - val_loss: 0.0511 - 275ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0472 - val_loss: 0.0571 - 297ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0575 - val_loss: 0.0496 - 289ms/epoch - 7ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0436 - val_loss: 0.0487 - 281ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0445 - val_loss: 0.0485 - 290ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0433 - val_loss: 0.0457 - 300ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0425 - val_loss: 0.0499 - 302ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0483 - val_loss: 0.0497 - 290ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0443 - val_loss: 0.0456 - 296ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0441 - 328ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0437 - 326ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0435 - 296ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0401 - val_loss: 0.0449 - 302ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0404 - val_loss: 0.0443 - 318ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0397 - val_loss: 0.0436 - 291ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0429 - 292ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 286ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0429 - 291ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0432 - 281ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0432 - 292ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0433 - 281ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0430 - 312ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0429 - 316ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0428 - 305ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0429 - 302ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0429 - 287ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0429 - 292ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 279ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 280ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0430 - 306ms/epoch - 8ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0428 - 274ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 319ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0429 - 300ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0431 - 293ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0428 - 308ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 296ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0428 - 292ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0428 - 282ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0430 - 281ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 282ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 280ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0430 - 274ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0428 - 282ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 280ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 269ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 303ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 293ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0427 - 274ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 281ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 279ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 280ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 271ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 279ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 283ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_37\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_38 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_37 (LSTM)              (None, 3, 97)             38412     \n",
      "                                                                 \n",
      " attention_layer_37 (Attenti  (None, 97)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 1)                 98        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,522\n",
      "Trainable params: 38,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2672 - val_loss: 0.0810 - 1s/epoch - 29ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0604 - val_loss: 0.0602 - 384ms/epoch - 10ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0527 - val_loss: 0.0578 - 300ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0518 - val_loss: 0.0580 - 309ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0563 - val_loss: 0.0624 - 308ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0605 - val_loss: 0.0572 - 357ms/epoch - 9ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0587 - val_loss: 0.0701 - 306ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0647 - val_loss: 0.0574 - 358ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0519 - val_loss: 0.0556 - 353ms/epoch - 9ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0506 - val_loss: 0.0536 - 334ms/epoch - 9ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0480 - val_loss: 0.0515 - 404ms/epoch - 10ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0470 - val_loss: 0.0532 - 337ms/epoch - 9ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0506 - val_loss: 0.0543 - 438ms/epoch - 11ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0627 - val_loss: 0.0640 - 311ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0526 - val_loss: 0.0498 - 329ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0436 - val_loss: 0.0473 - 287ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0416 - val_loss: 0.0466 - 277ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0403 - val_loss: 0.0449 - 305ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0473 - 350ms/epoch - 9ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0503 - val_loss: 0.0497 - 261ms/epoch - 7ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0462 - val_loss: 0.0521 - 281ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0479 - val_loss: 0.0453 - 337ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0436 - 343ms/epoch - 9ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0447 - 443ms/epoch - 11ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0403 - val_loss: 0.0442 - 288ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0406 - val_loss: 0.0439 - 334ms/epoch - 9ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0433 - 323ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0428 - 296ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0428 - 360ms/epoch - 9ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0429 - 294ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0434 - 310ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0438 - 299ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0434 - 294ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0430 - 294ms/epoch - 8ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0429 - 296ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 289ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0430 - 294ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0432 - 303ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0432 - 300ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0433 - 298ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0440 - 311ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0404 - val_loss: 0.0447 - 388ms/epoch - 10ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0473 - val_loss: 0.0789 - 293ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0935 - val_loss: 0.0618 - 283ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.2336 - val_loss: 0.0954 - 297ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.1341 - val_loss: 0.0620 - 349ms/epoch - 9ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0551 - val_loss: 0.0539 - 317ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0470 - val_loss: 0.0512 - 303ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0445 - val_loss: 0.0492 - 303ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0429 - val_loss: 0.0477 - 301ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0417 - val_loss: 0.0465 - 290ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0409 - val_loss: 0.0457 - 288ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0404 - val_loss: 0.0448 - 303ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0440 - 393ms/epoch - 10ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0435 - 361ms/epoch - 9ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0431 - 424ms/epoch - 11ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0428 - 309ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0426 - 288ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0425 - 274ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0423 - 282ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_39 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_38 (LSTM)              (None, 3, 76)             23712     \n",
      "                                                                 \n",
      " attention_layer_38 (Attenti  (None, 76)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 1)                 77        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,801\n",
      "Trainable params: 23,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.1940 - val_loss: 0.0678 - 1s/epoch - 27ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0554 - val_loss: 0.0591 - 297ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0526 - val_loss: 0.0576 - 279ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0536 - val_loss: 0.0607 - 270ms/epoch - 7ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0638 - val_loss: 0.0572 - 277ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0561 - val_loss: 0.0651 - 311ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0630 - val_loss: 0.0548 - 285ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0483 - val_loss: 0.0535 - 270ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0501 - val_loss: 0.0591 - 287ms/epoch - 7ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0533 - val_loss: 0.0526 - 279ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0455 - val_loss: 0.0495 - 315ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0480 - val_loss: 0.0591 - 296ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0672 - val_loss: 0.1132 - 279ms/epoch - 7ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0608 - val_loss: 0.0487 - 295ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0423 - val_loss: 0.0467 - 318ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0409 - val_loss: 0.0460 - 324ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0411 - val_loss: 0.0464 - 278ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0443 - 284ms/epoch - 7ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0469 - 318ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0471 - val_loss: 0.0443 - 281ms/epoch - 7ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0447 - 318ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0404 - val_loss: 0.0440 - 275ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0433 - 272ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0431 - 299ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0428 - 294ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0434 - 268ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0433 - 272ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0434 - 290ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0429 - 314ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0429 - 288ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0428 - 306ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 304ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0432 - 266ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0431 - 280ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0429 - 291ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0429 - 302ms/epoch - 8ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0429 - 273ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0429 - 266ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 307ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 282ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 260ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 277ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 295ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 261ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0429 - 266ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 265ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 284ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0429 - 349ms/epoch - 9ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0430 - 285ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 275ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 282ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0429 - 327ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0430 - 272ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0429 - 280ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0430 - 306ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0433 - 279ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0425 - 274ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0432 - 322ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 270ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0435 - 272ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_40 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_39 (LSTM)              (None, 3, 57)             13452     \n",
      "                                                                 \n",
      " attention_layer_39 (Attenti  (None, 57)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 58        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,522\n",
      "Trainable params: 13,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.4217 - val_loss: 0.0755 - 1s/epoch - 31ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0601 - val_loss: 0.0631 - 267ms/epoch - 7ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0559 - val_loss: 0.0623 - 280ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0550 - val_loss: 0.0616 - 311ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0543 - val_loss: 0.0609 - 279ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0536 - val_loss: 0.0603 - 350ms/epoch - 9ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0529 - val_loss: 0.0597 - 269ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0522 - val_loss: 0.0591 - 264ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0516 - val_loss: 0.0585 - 294ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0510 - val_loss: 0.0578 - 289ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0504 - val_loss: 0.0572 - 257ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0498 - val_loss: 0.0565 - 288ms/epoch - 7ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0491 - val_loss: 0.0558 - 281ms/epoch - 7ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0485 - val_loss: 0.0550 - 269ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0478 - val_loss: 0.0542 - 267ms/epoch - 7ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0471 - val_loss: 0.0534 - 354ms/epoch - 9ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0464 - val_loss: 0.0525 - 290ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0457 - val_loss: 0.0517 - 314ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0450 - val_loss: 0.0509 - 308ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0444 - val_loss: 0.0503 - 263ms/epoch - 7ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0442 - val_loss: 0.0499 - 270ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0437 - val_loss: 0.0491 - 333ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0427 - val_loss: 0.0483 - 267ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0418 - val_loss: 0.0477 - 284ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0414 - val_loss: 0.0473 - 322ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0409 - val_loss: 0.0468 - 282ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0404 - val_loss: 0.0464 - 273ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0401 - val_loss: 0.0461 - 321ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0458 - 264ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0397 - val_loss: 0.0456 - 261ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0399 - val_loss: 0.0457 - 271ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0403 - val_loss: 0.0455 - 342ms/epoch - 9ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0449 - 306ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0448 - 311ms/epoch - 8ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0446 - 267ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0444 - 268ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0442 - 309ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0441 - 274ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0440 - 328ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0439 - 428ms/epoch - 11ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0438 - 298ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0436 - 265ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0435 - 398ms/epoch - 10ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0434 - 262ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0434 - 262ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0433 - 348ms/epoch - 9ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0432 - 284ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0432 - 265ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0431 - 295ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0431 - 291ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0430 - 264ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0430 - 264ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0429 - 349ms/epoch - 9ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0429 - 282ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0429 - 353ms/epoch - 9ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0428 - 308ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0428 - 274ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0428 - 293ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0427 - 318ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 300ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "6 0.04232055468013068 [0.0050662737205928975, 76]\n",
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_41 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_40 (LSTM)              (None, 3, 70)             20160     \n",
      "                                                                 \n",
      " attention_layer_40 (Attenti  (None, 70)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 1)                 71        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,243\n",
      "Trainable params: 20,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.1765 - val_loss: 0.0717 - 1s/epoch - 29ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0589 - val_loss: 0.0589 - 297ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0531 - val_loss: 0.0614 - 288ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0674 - val_loss: 0.0596 - 310ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0527 - val_loss: 0.0562 - 284ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0518 - val_loss: 0.0567 - 314ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0533 - val_loss: 0.0542 - 283ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0497 - val_loss: 0.0557 - 267ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0579 - val_loss: 0.0564 - 296ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0514 - val_loss: 0.0548 - 270ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0495 - val_loss: 0.0513 - 266ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0452 - val_loss: 0.0494 - 302ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0440 - val_loss: 0.0483 - 333ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0433 - val_loss: 0.0471 - 269ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0456 - val_loss: 0.0522 - 268ms/epoch - 7ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0535 - val_loss: 0.0537 - 329ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0469 - val_loss: 0.0457 - 349ms/epoch - 9ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0401 - val_loss: 0.0446 - 261ms/epoch - 7ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0440 - 277ms/epoch - 7ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0436 - 333ms/epoch - 9ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0400 - val_loss: 0.0448 - 289ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0405 - val_loss: 0.0445 - 295ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0436 - 295ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0432 - 324ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0429 - 288ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0432 - 299ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0431 - 273ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0433 - 278ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0429 - 318ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 267ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0429 - 317ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 268ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0430 - 270ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 266ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0429 - 264ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0430 - 264ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0428 - 275ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0429 - 264ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0428 - 269ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 262ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0428 - 283ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 282ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 319ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 296ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 288ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0429 - 294ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 350ms/epoch - 9ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 281ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0435 - 313ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0435 - 308ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0401 - val_loss: 0.0451 - 308ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0409 - val_loss: 0.0450 - 297ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0415 - val_loss: 0.0562 - 297ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0842 - val_loss: 0.1666 - 292ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.1960 - val_loss: 0.1662 - 292ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0871 - val_loss: 0.0580 - 288ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0495 - val_loss: 0.0535 - 294ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0459 - val_loss: 0.0516 - 294ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0441 - val_loss: 0.0497 - 297ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0424 - val_loss: 0.0475 - 328ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_42 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_41 (LSTM)              (None, 3, 45)             8460      \n",
      "                                                                 \n",
      " attention_layer_41 (Attenti  (None, 45)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 1)                 46        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,518\n",
      "Trainable params: 8,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2458 - val_loss: 0.0651 - 1s/epoch - 29ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0561 - val_loss: 0.0614 - 290ms/epoch - 7ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0538 - val_loss: 0.0601 - 285ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0523 - val_loss: 0.0584 - 272ms/epoch - 7ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0512 - val_loss: 0.0571 - 274ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0513 - val_loss: 0.0584 - 291ms/epoch - 7ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0538 - val_loss: 0.0571 - 271ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0500 - val_loss: 0.0539 - 270ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0476 - val_loss: 0.0532 - 282ms/epoch - 7ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0476 - val_loss: 0.0531 - 268ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0473 - val_loss: 0.0513 - 271ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0451 - val_loss: 0.0496 - 284ms/epoch - 7ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0437 - val_loss: 0.0488 - 279ms/epoch - 7ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0426 - val_loss: 0.0475 - 285ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0412 - val_loss: 0.0471 - 278ms/epoch - 7ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0464 - val_loss: 0.0539 - 281ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0525 - val_loss: 0.0507 - 273ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0419 - val_loss: 0.0454 - 281ms/epoch - 7ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0447 - 279ms/epoch - 7ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0454 - 274ms/epoch - 7ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0405 - val_loss: 0.0448 - 286ms/epoch - 7ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0442 - 279ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0443 - 300ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0437 - 279ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0437 - 278ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0433 - 279ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0435 - 278ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0431 - 281ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 285ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0429 - 278ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0428 - 282ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 279ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 274ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 279ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 275ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0425 - 283ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0425 - 279ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0425 - 295ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0424 - 283ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0424 - 306ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0424 - 269ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0423 - 276ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0423 - 274ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0424 - 284ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0423 - 285ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0423 - 285ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0423 - 280ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 287ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0422 - 281ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0423 - 280ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0423 - 268ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0423 - 280ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0423 - 280ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0423 - 271ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0423 - 327ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0424 - 375ms/epoch - 10ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0423 - 320ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0423 - 288ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0423 - 325ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0423 - 333ms/epoch - 9ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_42\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_43 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_42 (LSTM)              (None, 3, 27)             3132      \n",
      "                                                                 \n",
      " attention_layer_42 (Attenti  (None, 27)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 1)                 28        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,172\n",
      "Trainable params: 3,172\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.3454 - val_loss: 0.0694 - 1s/epoch - 30ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0581 - val_loss: 0.0619 - 353ms/epoch - 9ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0550 - val_loss: 0.0602 - 315ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0532 - val_loss: 0.0589 - 282ms/epoch - 7ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0516 - val_loss: 0.0574 - 323ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0503 - val_loss: 0.0560 - 296ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0490 - val_loss: 0.0545 - 281ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0478 - val_loss: 0.0529 - 314ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0468 - val_loss: 0.0518 - 317ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0470 - val_loss: 0.0520 - 305ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0476 - val_loss: 0.0499 - 342ms/epoch - 9ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0442 - val_loss: 0.0481 - 291ms/epoch - 7ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0440 - val_loss: 0.0504 - 354ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0436 - val_loss: 0.0463 - 330ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0409 - val_loss: 0.0478 - 296ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0486 - val_loss: 0.0500 - 291ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0433 - val_loss: 0.0450 - 286ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0447 - 296ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0455 - 300ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0409 - val_loss: 0.0455 - 307ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0401 - val_loss: 0.0442 - 361ms/epoch - 9ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0438 - 294ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0440 - 279ms/epoch - 7ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0445 - 282ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0399 - val_loss: 0.0440 - 286ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0436 - 274ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0436 - 302ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0435 - 287ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0433 - 286ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 287ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 287ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 305ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0431 - 310ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0430 - 320ms/epoch - 8ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0428 - 313ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 313ms/epoch - 8ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 292ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 311ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 287ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 302ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 278ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 333ms/epoch - 9ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 313ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 293ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 278ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 283ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 269ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 274ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 283ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 314ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 331ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 302ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 304ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 288ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 282ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 280ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 289ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 369ms/epoch - 9ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 310ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0425 - 280ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_43\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_44 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_43 (LSTM)              (None, 3, 35)             5180      \n",
      "                                                                 \n",
      " attention_layer_43 (Attenti  (None, 35)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,228\n",
      "Trainable params: 5,228\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2412 - val_loss: 0.0728 - 1s/epoch - 32ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0584 - val_loss: 0.0595 - 301ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0585 - val_loss: 0.0843 - 286ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0755 - val_loss: 0.0609 - 296ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0513 - val_loss: 0.0540 - 291ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0491 - val_loss: 0.0570 - 299ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0599 - val_loss: 0.0529 - 306ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0515 - val_loss: 0.0639 - 288ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0631 - val_loss: 0.0516 - 290ms/epoch - 7ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0442 - val_loss: 0.0505 - 302ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0460 - val_loss: 0.0497 - 296ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0448 - val_loss: 0.0492 - 294ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0439 - val_loss: 0.0484 - 311ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0427 - val_loss: 0.0492 - 304ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0504 - val_loss: 0.0454 - 284ms/epoch - 7ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0411 - val_loss: 0.0471 - 303ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0432 - val_loss: 0.0447 - 295ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0442 - 306ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0438 - 304ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0434 - 306ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0434 - 302ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0429 - 286ms/epoch - 7ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0434 - 319ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0433 - 296ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0430 - 316ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0431 - 331ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0429 - 329ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 321ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0431 - 306ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0431 - 317ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0432 - 290ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0431 - 287ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0433 - 301ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0430 - 310ms/epoch - 8ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 298ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 285ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0432 - 299ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0432 - 288ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0436 - 293ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0435 - 287ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0436 - 291ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0431 - 301ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0433 - 283ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 293ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0432 - 295ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0432 - 304ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0434 - 310ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0431 - 296ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0432 - 296ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 310ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0433 - 295ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0432 - 294ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 298ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 316ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 306ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0433 - 294ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0430 - 296ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0430 - 305ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0433 - 285ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0437 - 300ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_44\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_45 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_44 (LSTM)              (None, 3, 15)             1020      \n",
      "                                                                 \n",
      " attention_layer_44 (Attenti  (None, 15)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,048\n",
      "Trainable params: 1,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2433 - val_loss: 0.0753 - 1s/epoch - 28ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0606 - val_loss: 0.0633 - 309ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0548 - val_loss: 0.0608 - 308ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0522 - val_loss: 0.0581 - 311ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0502 - val_loss: 0.0558 - 306ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0486 - val_loss: 0.0540 - 308ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0482 - val_loss: 0.0541 - 319ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0504 - val_loss: 0.0550 - 306ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0491 - val_loss: 0.0498 - 308ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0438 - val_loss: 0.0529 - 332ms/epoch - 9ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0497 - val_loss: 0.0530 - 335ms/epoch - 9ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0453 - val_loss: 0.0474 - 319ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0411 - val_loss: 0.0460 - 309ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0415 - val_loss: 0.0473 - 307ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0441 - val_loss: 0.0462 - 303ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0430 - val_loss: 0.0479 - 323ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0434 - val_loss: 0.0446 - 326ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0438 - 317ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0446 - 321ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0406 - val_loss: 0.0447 - 308ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0443 - 327ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0436 - 294ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0436 - 293ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0435 - 298ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0431 - 304ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 295ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0432 - 302ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0431 - 330ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0430 - 301ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 284ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0428 - 274ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0428 - 289ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0427 - 267ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 275ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 276ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 289ms/epoch - 7ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0425 - 289ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0425 - 282ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 270ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0425 - 287ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0427 - 276ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 296ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0426 - 295ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 290ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 314ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 317ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 287ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0427 - 286ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0425 - 289ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 282ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 281ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 287ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 281ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0427 - 288ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0426 - 293ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 287ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 279ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 279ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0427 - 261ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 268ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "7 0.04232055468013068 [0.009302589510019284, 35]\n",
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_45\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_46 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_45 (LSTM)              (None, 3, 18)             1440      \n",
      "                                                                 \n",
      " attention_layer_45 (Attenti  (None, 18)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 1)                 19        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,471\n",
      "Trainable params: 1,471\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.4650 - val_loss: 0.0814 - 1s/epoch - 27ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0666 - val_loss: 0.0649 - 347ms/epoch - 9ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0582 - val_loss: 0.0629 - 385ms/epoch - 10ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0560 - val_loss: 0.0614 - 308ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0541 - val_loss: 0.0600 - 306ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0525 - val_loss: 0.0587 - 295ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0512 - val_loss: 0.0575 - 290ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0501 - val_loss: 0.0564 - 339ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0490 - val_loss: 0.0552 - 315ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0480 - val_loss: 0.0539 - 298ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0468 - val_loss: 0.0525 - 297ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0457 - val_loss: 0.0513 - 292ms/epoch - 7ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0447 - val_loss: 0.0502 - 337ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0439 - val_loss: 0.0493 - 299ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0432 - val_loss: 0.0485 - 304ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0427 - val_loss: 0.0478 - 291ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0419 - val_loss: 0.0469 - 292ms/epoch - 7ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0410 - val_loss: 0.0463 - 289ms/epoch - 7ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0403 - val_loss: 0.0460 - 303ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0401 - val_loss: 0.0463 - 312ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0454 - 309ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0444 - 322ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0451 - 304ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0424 - val_loss: 0.0492 - 347ms/epoch - 9ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0430 - val_loss: 0.0439 - 294ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0436 - 310ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0439 - 303ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0443 - 292ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0440 - 291ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0436 - 315ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0435 - 278ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0435 - 285ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0434 - 302ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0433 - 278ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0433 - 276ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0432 - 301ms/epoch - 8ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0430 - 287ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0430 - 283ms/epoch - 7ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0430 - 298ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0430 - 293ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0429 - 272ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0429 - 281ms/epoch - 7ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0429 - 283ms/epoch - 7ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0428 - 297ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0428 - 290ms/epoch - 7ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0427 - 289ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0428 - 295ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0428 - 295ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0428 - 289ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 286ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0428 - 285ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 328ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0427 - 294ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0427 - 302ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0427 - 285ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0427 - 288ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0427 - 304ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0427 - 288ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0426 - 296ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0426 - 294ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_46\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_47 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_46 (LSTM)              (None, 3, 42)             7392      \n",
      "                                                                 \n",
      " attention_layer_46 (Attenti  (None, 42)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 1)                 43        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,447\n",
      "Trainable params: 7,447\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.3247 - val_loss: 0.0691 - 1s/epoch - 32ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0579 - val_loss: 0.0631 - 297ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0552 - val_loss: 0.0615 - 294ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0538 - val_loss: 0.0604 - 295ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0527 - val_loss: 0.0592 - 283ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0517 - val_loss: 0.0582 - 298ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0508 - val_loss: 0.0571 - 296ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0501 - val_loss: 0.0562 - 283ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0499 - val_loss: 0.0560 - 319ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0507 - val_loss: 0.0564 - 297ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0505 - val_loss: 0.0545 - 286ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0481 - val_loss: 0.0530 - 308ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0470 - val_loss: 0.0525 - 352ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0470 - val_loss: 0.0521 - 282ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0463 - val_loss: 0.0505 - 299ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0444 - val_loss: 0.0492 - 290ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0433 - val_loss: 0.0482 - 310ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0426 - val_loss: 0.0474 - 302ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0420 - val_loss: 0.0466 - 295ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0418 - val_loss: 0.0468 - 340ms/epoch - 9ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0433 - val_loss: 0.0476 - 301ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0423 - val_loss: 0.0459 - 318ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0400 - val_loss: 0.0450 - 315ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0447 - 320ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0444 - 363ms/epoch - 9ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0443 - 279ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0442 - 345ms/epoch - 9ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0439 - 311ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0437 - 264ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0436 - 290ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0435 - 312ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0434 - 299ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0432 - 264ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0431 - 268ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 351ms/epoch - 9ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 308ms/epoch - 8ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0429 - 293ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0428 - 300ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0428 - 326ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0427 - 351ms/epoch - 9ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0427 - 314ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 298ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 317ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0426 - 331ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 313ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 285ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 287ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 289ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0425 - 283ms/epoch - 7ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 287ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 310ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0425 - 296ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0425 - 288ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0425 - 287ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0425 - 296ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0425 - 371ms/epoch - 10ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0425 - 326ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0425 - 345ms/epoch - 9ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0425 - 356ms/epoch - 9ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0425 - 310ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_47\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_48 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_47 (LSTM)              (None, 3, 46)             8832      \n",
      "                                                                 \n",
      " attention_layer_47 (Attenti  (None, 46)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 1)                 47        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,891\n",
      "Trainable params: 8,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 2s - loss: 0.4253 - val_loss: 0.0816 - 2s/epoch - 39ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0606 - val_loss: 0.0639 - 394ms/epoch - 10ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0572 - val_loss: 0.0629 - 325ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0559 - val_loss: 0.0620 - 330ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0549 - val_loss: 0.0611 - 345ms/epoch - 9ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0539 - val_loss: 0.0603 - 310ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0531 - val_loss: 0.0596 - 339ms/epoch - 9ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0523 - val_loss: 0.0589 - 433ms/epoch - 11ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0515 - val_loss: 0.0583 - 349ms/epoch - 9ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0508 - val_loss: 0.0576 - 348ms/epoch - 9ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0502 - val_loss: 0.0570 - 353ms/epoch - 9ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0495 - val_loss: 0.0563 - 394ms/epoch - 10ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0488 - val_loss: 0.0556 - 332ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0481 - val_loss: 0.0548 - 355ms/epoch - 9ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0474 - val_loss: 0.0541 - 415ms/epoch - 11ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0467 - val_loss: 0.0533 - 393ms/epoch - 10ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0460 - val_loss: 0.0525 - 387ms/epoch - 10ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0453 - val_loss: 0.0517 - 370ms/epoch - 9ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0445 - val_loss: 0.0508 - 342ms/epoch - 9ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0438 - val_loss: 0.0500 - 395ms/epoch - 10ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0431 - val_loss: 0.0493 - 402ms/epoch - 10ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0426 - val_loss: 0.0487 - 355ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0423 - val_loss: 0.0482 - 331ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0417 - val_loss: 0.0475 - 336ms/epoch - 9ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0409 - val_loss: 0.0468 - 291ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0403 - val_loss: 0.0464 - 301ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0400 - val_loss: 0.0462 - 323ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0397 - val_loss: 0.0458 - 323ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0454 - 320ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0451 - 301ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0449 - 314ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0447 - 304ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0445 - 342ms/epoch - 9ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0442 - 347ms/epoch - 9ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0441 - 340ms/epoch - 9ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0439 - 347ms/epoch - 9ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0438 - 326ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0437 - 324ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0435 - 331ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0434 - 348ms/epoch - 9ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0434 - 284ms/epoch - 7ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0433 - 354ms/epoch - 9ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0432 - 349ms/epoch - 9ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0431 - 344ms/epoch - 9ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0431 - 310ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0430 - 420ms/epoch - 11ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0430 - 327ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0429 - 349ms/epoch - 9ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0429 - 323ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0429 - 328ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0428 - 335ms/epoch - 9ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0428 - 314ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0427 - 324ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0427 - 327ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0427 - 334ms/epoch - 9ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0427 - 328ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0426 - 324ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0426 - 311ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0426 - 324ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0426 - 291ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_48\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_49 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_48 (LSTM)              (None, 3, 41)             7052      \n",
      "                                                                 \n",
      " attention_layer_48 (Attenti  (None, 41)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 1)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,106\n",
      "Trainable params: 7,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2685 - val_loss: 0.1096 - 1s/epoch - 31ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0764 - val_loss: 0.0629 - 366ms/epoch - 9ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0544 - val_loss: 0.0568 - 341ms/epoch - 9ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0535 - val_loss: 0.0664 - 334ms/epoch - 9ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0676 - val_loss: 0.0778 - 333ms/epoch - 9ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0628 - val_loss: 0.0552 - 381ms/epoch - 10ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0475 - val_loss: 0.0525 - 353ms/epoch - 9ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0526 - val_loss: 0.0679 - 339ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0665 - val_loss: 0.0749 - 419ms/epoch - 11ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0649 - val_loss: 0.0540 - 342ms/epoch - 9ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0454 - val_loss: 0.0490 - 334ms/epoch - 9ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0436 - val_loss: 0.0479 - 341ms/epoch - 9ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0433 - val_loss: 0.0524 - 328ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0459 - val_loss: 0.0482 - 342ms/epoch - 9ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0415 - val_loss: 0.0446 - 338ms/epoch - 9ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0435 - val_loss: 0.0524 - 324ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0460 - val_loss: 0.0448 - 329ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0406 - val_loss: 0.0449 - 334ms/epoch - 9ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0431 - 329ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0433 - 336ms/epoch - 9ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0434 - 334ms/epoch - 9ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0434 - 349ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0442 - 337ms/epoch - 9ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0400 - val_loss: 0.0435 - 349ms/epoch - 9ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0434 - 341ms/epoch - 9ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0430 - 360ms/epoch - 9ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0431 - 311ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0432 - 310ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0434 - 341ms/epoch - 9ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0434 - 302ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0431 - 333ms/epoch - 9ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0434 - 330ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0432 - 461ms/epoch - 12ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0431 - 334ms/epoch - 9ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0433 - 370ms/epoch - 9ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0433 - 340ms/epoch - 9ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0433 - 375ms/epoch - 10ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0434 - 319ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0437 - 328ms/epoch - 8ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0431 - 289ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0433 - 315ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0433 - 347ms/epoch - 9ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0434 - 328ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0433 - 366ms/epoch - 9ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0435 - 305ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0433 - 327ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0433 - 305ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0433 - 316ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0434 - 330ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0436 - 351ms/epoch - 9ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0441 - 323ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0400 - val_loss: 0.0436 - 328ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0432 - 320ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0432 - 322ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0435 - 320ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0431 - 388ms/epoch - 10ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0433 - 374ms/epoch - 10ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 330ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0434 - 360ms/epoch - 9ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0435 - 329ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_49\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_50 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_49 (LSTM)              (None, 3, 25)             2700      \n",
      "                                                                 \n",
      " attention_layer_49 (Attenti  (None, 25)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 1)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,738\n",
      "Trainable params: 2,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.3102 - val_loss: 0.0715 - 1s/epoch - 34ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0577 - val_loss: 0.0617 - 340ms/epoch - 9ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0542 - val_loss: 0.0598 - 345ms/epoch - 9ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0524 - val_loss: 0.0582 - 334ms/epoch - 9ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0508 - val_loss: 0.0566 - 349ms/epoch - 9ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0494 - val_loss: 0.0551 - 322ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0484 - val_loss: 0.0540 - 325ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0485 - val_loss: 0.0547 - 358ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0501 - val_loss: 0.0536 - 381ms/epoch - 10ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0473 - val_loss: 0.0513 - 323ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0452 - val_loss: 0.0505 - 371ms/epoch - 10ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0450 - val_loss: 0.0500 - 352ms/epoch - 9ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0446 - val_loss: 0.0497 - 335ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0439 - val_loss: 0.0477 - 347ms/epoch - 9ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0411 - val_loss: 0.0469 - 335ms/epoch - 9ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0446 - val_loss: 0.0561 - 346ms/epoch - 9ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0498 - val_loss: 0.0462 - 327ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0397 - val_loss: 0.0456 - 331ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0457 - 348ms/epoch - 9ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0455 - 301ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0448 - 353ms/epoch - 9ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0448 - 315ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0449 - 338ms/epoch - 9ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0444 - 318ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0442 - 320ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0440 - 333ms/epoch - 9ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0439 - 378ms/epoch - 10ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0437 - 333ms/epoch - 9ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0435 - 354ms/epoch - 9ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0434 - 336ms/epoch - 9ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0433 - 359ms/epoch - 9ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0431 - 308ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0431 - 341ms/epoch - 9ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0430 - 346ms/epoch - 9ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0430 - 359ms/epoch - 9ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 380ms/epoch - 10ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0429 - 358ms/epoch - 9ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 376ms/epoch - 10ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0428 - 335ms/epoch - 9ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0428 - 312ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0427 - 342ms/epoch - 9ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 346ms/epoch - 9ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 324ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 336ms/epoch - 9ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 357ms/epoch - 9ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0427 - 364ms/epoch - 9ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 366ms/epoch - 9ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0427 - 398ms/epoch - 10ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0426 - 428ms/epoch - 11ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0427 - 434ms/epoch - 11ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0426 - 286ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0426 - 363ms/epoch - 9ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0426 - 344ms/epoch - 9ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0426 - 363ms/epoch - 9ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0427 - 372ms/epoch - 10ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 383ms/epoch - 10ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 385ms/epoch - 10ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0427 - 351ms/epoch - 9ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0428 - 402ms/epoch - 10ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0428 - 396ms/epoch - 10ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "8 0.04232055468013068 [0.009923348051118237, 41]\n",
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_50\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_51 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_50 (LSTM)              (None, 3, 14)             896       \n",
      "                                                                 \n",
      " attention_layer_50 (Attenti  (None, 14)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 1)                 15        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 923\n",
      "Trainable params: 923\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.3904 - val_loss: 0.0773 - 1s/epoch - 28ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0666 - val_loss: 0.0630 - 285ms/epoch - 7ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0558 - val_loss: 0.0606 - 296ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0534 - val_loss: 0.0592 - 305ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0519 - val_loss: 0.0582 - 297ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0507 - val_loss: 0.0572 - 289ms/epoch - 7ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0496 - val_loss: 0.0563 - 304ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0487 - val_loss: 0.0554 - 308ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0479 - val_loss: 0.0546 - 296ms/epoch - 8ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0471 - val_loss: 0.0538 - 292ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0464 - val_loss: 0.0530 - 303ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0457 - val_loss: 0.0522 - 295ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0450 - val_loss: 0.0515 - 292ms/epoch - 7ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0444 - val_loss: 0.0507 - 278ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0437 - val_loss: 0.0500 - 334ms/epoch - 9ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0431 - val_loss: 0.0493 - 288ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0425 - val_loss: 0.0486 - 310ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0419 - val_loss: 0.0480 - 339ms/epoch - 9ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0413 - val_loss: 0.0473 - 278ms/epoch - 7ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0408 - val_loss: 0.0468 - 284ms/epoch - 7ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0404 - val_loss: 0.0462 - 319ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0399 - val_loss: 0.0458 - 296ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0453 - 345ms/epoch - 9ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0450 - 294ms/epoch - 8ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0445 - 285ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0442 - 439ms/epoch - 11ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0440 - 435ms/epoch - 11ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0438 - 304ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0436 - 314ms/epoch - 8ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0434 - 278ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0432 - 315ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0431 - 296ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0431 - 259ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0429 - 280ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 272ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 335ms/epoch - 9ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0367 - val_loss: 0.0426 - 282ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 328ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 289ms/epoch - 7ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 284ms/epoch - 7ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0366 - val_loss: 0.0426 - 296ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0365 - val_loss: 0.0425 - 296ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0425 - 322ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0425 - 292ms/epoch - 7ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0364 - val_loss: 0.0424 - 306ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0363 - val_loss: 0.0424 - 286ms/epoch - 7ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0424 - 274ms/epoch - 7ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0424 - 315ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0424 - 327ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0424 - 269ms/epoch - 7ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0362 - val_loss: 0.0424 - 279ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0361 - val_loss: 0.0424 - 281ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0361 - val_loss: 0.0424 - 271ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0361 - val_loss: 0.0424 - 263ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0361 - val_loss: 0.0424 - 274ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0361 - val_loss: 0.0424 - 272ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0360 - val_loss: 0.0424 - 263ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0360 - val_loss: 0.0424 - 262ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0360 - val_loss: 0.0424 - 345ms/epoch - 9ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0360 - val_loss: 0.0424 - 283ms/epoch - 7ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_52 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_51 (LSTM)              (None, 3, 43)             7740      \n",
      "                                                                 \n",
      " attention_layer_51 (Attenti  (None, 43)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 1)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,796\n",
      "Trainable params: 7,796\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2114 - val_loss: 0.0694 - 1s/epoch - 28ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0831 - val_loss: 0.0642 - 276ms/epoch - 7ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0542 - val_loss: 0.0571 - 287ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0505 - val_loss: 0.0577 - 308ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0653 - val_loss: 0.0682 - 451ms/epoch - 12ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0620 - val_loss: 0.0583 - 363ms/epoch - 9ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0552 - val_loss: 0.0546 - 329ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0487 - val_loss: 0.0518 - 368ms/epoch - 9ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0466 - val_loss: 0.0513 - 492ms/epoch - 13ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0454 - val_loss: 0.0530 - 321ms/epoch - 8ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0529 - val_loss: 0.0509 - 454ms/epoch - 12ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0557 - val_loss: 0.0678 - 263ms/epoch - 7ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0540 - val_loss: 0.0475 - 329ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0411 - val_loss: 0.0449 - 279ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0393 - val_loss: 0.0442 - 275ms/epoch - 7ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0436 - 355ms/epoch - 9ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0434 - 317ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0436 - val_loss: 0.0484 - 276ms/epoch - 7ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0426 - val_loss: 0.0441 - 283ms/epoch - 7ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0434 - 307ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0430 - 297ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0430 - 380ms/epoch - 10ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0435 - 451ms/epoch - 12ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0433 - 278ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0432 - 440ms/epoch - 11ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0429 - 443ms/epoch - 11ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 291ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0429 - 279ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0431 - 281ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0432 - 267ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0432 - 270ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0432 - 289ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0434 - 301ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0433 - 288ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0434 - 318ms/epoch - 8ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0433 - 296ms/epoch - 8ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0434 - 276ms/epoch - 7ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0431 - 298ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0433 - 408ms/epoch - 10ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0435 - 324ms/epoch - 8ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0434 - 314ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0435 - 369ms/epoch - 9ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0432 - 474ms/epoch - 12ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0431 - 365ms/epoch - 9ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0432 - 392ms/epoch - 10ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0436 - 299ms/epoch - 8ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0432 - 364ms/epoch - 9ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 329ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0432 - 308ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0434 - 320ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0432 - 269ms/epoch - 7ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0431 - 277ms/epoch - 7ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0433 - 267ms/epoch - 7ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0432 - 273ms/epoch - 7ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0432 - 279ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0434 - 272ms/epoch - 7ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0429 - 268ms/epoch - 7ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 273ms/epoch - 7ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 256ms/epoch - 7ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 301ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_52\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_53 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_52 (LSTM)              (None, 3, 21)             1932      \n",
      "                                                                 \n",
      " attention_layer_52 (Attenti  (None, 21)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,966\n",
      "Trainable params: 1,966\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2361 - val_loss: 0.0655 - 1s/epoch - 31ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0548 - val_loss: 0.0601 - 278ms/epoch - 7ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0521 - val_loss: 0.0582 - 283ms/epoch - 7ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0504 - val_loss: 0.0562 - 296ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0491 - val_loss: 0.0552 - 274ms/epoch - 7ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0511 - val_loss: 0.0602 - 285ms/epoch - 7ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0548 - val_loss: 0.0521 - 279ms/epoch - 7ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0452 - val_loss: 0.0504 - 289ms/epoch - 7ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0452 - val_loss: 0.0510 - 278ms/epoch - 7ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0475 - val_loss: 0.0540 - 283ms/epoch - 7ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0466 - val_loss: 0.0472 - 268ms/epoch - 7ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0407 - val_loss: 0.0461 - 310ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0430 - val_loss: 0.0545 - 350ms/epoch - 9ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0501 - val_loss: 0.0460 - 276ms/epoch - 7ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0397 - val_loss: 0.0449 - 278ms/epoch - 7ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0453 - 268ms/epoch - 7ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0446 - 314ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0447 - 324ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0443 - 321ms/epoch - 8ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0441 - 497ms/epoch - 13ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0438 - 372ms/epoch - 10ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0438 - 300ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0436 - 480ms/epoch - 12ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0436 - 267ms/epoch - 7ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0381 - val_loss: 0.0434 - 280ms/epoch - 7ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0434 - 266ms/epoch - 7ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0377 - val_loss: 0.0433 - 271ms/epoch - 7ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0432 - 273ms/epoch - 7ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0433 - 271ms/epoch - 7ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0431 - 278ms/epoch - 7ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0431 - 275ms/epoch - 7ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0430 - 274ms/epoch - 7ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0431 - 274ms/epoch - 7ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 271ms/epoch - 7ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0429 - 266ms/epoch - 7ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 325ms/epoch - 8ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0430 - 308ms/epoch - 8ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0429 - 316ms/epoch - 8ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0430 - 344ms/epoch - 9ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0371 - val_loss: 0.0428 - 340ms/epoch - 9ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0430 - 296ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0427 - 318ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0428 - 331ms/epoch - 8ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 368ms/epoch - 9ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0428 - 324ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 352ms/epoch - 9ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 301ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0428 - 284ms/epoch - 7ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0428 - 341ms/epoch - 9ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 330ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 322ms/epoch - 8ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 316ms/epoch - 8ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 337ms/epoch - 9ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 314ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 370ms/epoch - 9ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0428 - 326ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 310ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0427 - 340ms/epoch - 9ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 354ms/epoch - 9ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0369 - val_loss: 0.0427 - 310ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_53 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_54 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_53 (LSTM)              (None, 3, 89)             32396     \n",
      "                                                                 \n",
      " attention_layer_53 (Attenti  (None, 89)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 1)                 90        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,498\n",
      "Trainable params: 32,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.2487 - val_loss: 0.0646 - 1s/epoch - 32ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0597 - val_loss: 0.0621 - 363ms/epoch - 9ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0537 - val_loss: 0.0593 - 357ms/epoch - 9ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0534 - val_loss: 0.0604 - 331ms/epoch - 8ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0588 - val_loss: 0.0655 - 331ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0584 - val_loss: 0.0581 - 378ms/epoch - 10ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0519 - val_loss: 0.0570 - 333ms/epoch - 9ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0521 - val_loss: 0.0560 - 326ms/epoch - 8ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0513 - val_loss: 0.0543 - 349ms/epoch - 9ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0507 - val_loss: 0.0567 - 356ms/epoch - 9ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0589 - val_loss: 0.0641 - 373ms/epoch - 10ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0580 - val_loss: 0.0563 - 377ms/epoch - 10ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0485 - val_loss: 0.0517 - 420ms/epoch - 11ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0452 - val_loss: 0.0504 - 401ms/epoch - 10ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0443 - val_loss: 0.0501 - 365ms/epoch - 9ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0432 - val_loss: 0.0485 - 356ms/epoch - 9ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0424 - val_loss: 0.0524 - 359ms/epoch - 9ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0602 - val_loss: 0.0505 - 305ms/epoch - 8ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0444 - val_loss: 0.0469 - 291ms/epoch - 7ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0408 - val_loss: 0.0465 - 321ms/epoch - 8ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0428 - val_loss: 0.0465 - 325ms/epoch - 8ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0419 - val_loss: 0.0466 - 311ms/epoch - 8ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0427 - val_loss: 0.0455 - 352ms/epoch - 9ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0446 - 338ms/epoch - 9ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0398 - val_loss: 0.0440 - 326ms/epoch - 8ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0440 - 317ms/epoch - 8ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0438 - 365ms/epoch - 9ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0438 - 328ms/epoch - 8ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0390 - val_loss: 0.0432 - 356ms/epoch - 9ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0433 - 356ms/epoch - 9ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0382 - val_loss: 0.0429 - 331ms/epoch - 8ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 314ms/epoch - 8ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0380 - val_loss: 0.0429 - 307ms/epoch - 8ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0376 - val_loss: 0.0431 - 311ms/epoch - 8ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0430 - 350ms/epoch - 9ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 404ms/epoch - 10ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0429 - 350ms/epoch - 9ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 363ms/epoch - 9ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0427 - 336ms/epoch - 9ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0428 - 338ms/epoch - 9ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 330ms/epoch - 8ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 316ms/epoch - 8ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0427 - 337ms/epoch - 9ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0427 - 345ms/epoch - 9ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0427 - 340ms/epoch - 9ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0375 - val_loss: 0.0428 - 335ms/epoch - 9ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0427 - 329ms/epoch - 8ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0427 - 320ms/epoch - 8ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0428 - 330ms/epoch - 8ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 390ms/epoch - 10ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0427 - 346ms/epoch - 9ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0426 - 340ms/epoch - 9ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0373 - val_loss: 0.0427 - 312ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0374 - val_loss: 0.0429 - 330ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0378 - val_loss: 0.0433 - 321ms/epoch - 8ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0442 - 361ms/epoch - 9ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0426 - 368ms/epoch - 9ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0372 - val_loss: 0.0425 - 343ms/epoch - 9ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0368 - val_loss: 0.0426 - 347ms/epoch - 9ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0370 - val_loss: 0.0426 - 312ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_54\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_55 (InputLayer)       [(None, 3, 1)]            0         \n",
      "                                                                 \n",
      " lstm_54 (LSTM)              (None, 3, 76)             23712     \n",
      "                                                                 \n",
      " attention_layer_54 (Attenti  (None, 76)               12        \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 1)                 77        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,801\n",
      "Trainable params: 23,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "39/39 - 1s - loss: 0.3954 - val_loss: 0.0801 - 1s/epoch - 30ms/step\n",
      "Epoch 2/60\n",
      "39/39 - 0s - loss: 0.0690 - val_loss: 0.0637 - 326ms/epoch - 8ms/step\n",
      "Epoch 3/60\n",
      "39/39 - 0s - loss: 0.0555 - val_loss: 0.0599 - 323ms/epoch - 8ms/step\n",
      "Epoch 4/60\n",
      "39/39 - 0s - loss: 0.0552 - val_loss: 0.0633 - 346ms/epoch - 9ms/step\n",
      "Epoch 5/60\n",
      "39/39 - 0s - loss: 0.0654 - val_loss: 0.0607 - 306ms/epoch - 8ms/step\n",
      "Epoch 6/60\n",
      "39/39 - 0s - loss: 0.0553 - val_loss: 0.0568 - 330ms/epoch - 8ms/step\n",
      "Epoch 7/60\n",
      "39/39 - 0s - loss: 0.0514 - val_loss: 0.0568 - 295ms/epoch - 8ms/step\n",
      "Epoch 8/60\n",
      "39/39 - 0s - loss: 0.0526 - val_loss: 0.0613 - 374ms/epoch - 10ms/step\n",
      "Epoch 9/60\n",
      "39/39 - 0s - loss: 0.0672 - val_loss: 0.0675 - 398ms/epoch - 10ms/step\n",
      "Epoch 10/60\n",
      "39/39 - 0s - loss: 0.0580 - val_loss: 0.0611 - 361ms/epoch - 9ms/step\n",
      "Epoch 11/60\n",
      "39/39 - 0s - loss: 0.0581 - val_loss: 0.0531 - 305ms/epoch - 8ms/step\n",
      "Epoch 12/60\n",
      "39/39 - 0s - loss: 0.0475 - val_loss: 0.0506 - 318ms/epoch - 8ms/step\n",
      "Epoch 13/60\n",
      "39/39 - 0s - loss: 0.0441 - val_loss: 0.0499 - 301ms/epoch - 8ms/step\n",
      "Epoch 14/60\n",
      "39/39 - 0s - loss: 0.0503 - val_loss: 0.0524 - 306ms/epoch - 8ms/step\n",
      "Epoch 15/60\n",
      "39/39 - 0s - loss: 0.0556 - val_loss: 0.0514 - 329ms/epoch - 8ms/step\n",
      "Epoch 16/60\n",
      "39/39 - 0s - loss: 0.0441 - val_loss: 0.0456 - 325ms/epoch - 8ms/step\n",
      "Epoch 17/60\n",
      "39/39 - 0s - loss: 0.0397 - val_loss: 0.0446 - 330ms/epoch - 8ms/step\n",
      "Epoch 18/60\n",
      "39/39 - 0s - loss: 0.0488 - val_loss: 0.0533 - 346ms/epoch - 9ms/step\n",
      "Epoch 19/60\n",
      "39/39 - 0s - loss: 0.0567 - val_loss: 0.0455 - 349ms/epoch - 9ms/step\n",
      "Epoch 20/60\n",
      "39/39 - 0s - loss: 0.0573 - val_loss: 0.1104 - 335ms/epoch - 9ms/step\n",
      "Epoch 21/60\n",
      "39/39 - 0s - loss: 0.0608 - val_loss: 0.0446 - 376ms/epoch - 10ms/step\n",
      "Epoch 22/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0441 - 343ms/epoch - 9ms/step\n",
      "Epoch 23/60\n",
      "39/39 - 0s - loss: 0.0403 - val_loss: 0.0463 - 316ms/epoch - 8ms/step\n",
      "Epoch 24/60\n",
      "39/39 - 0s - loss: 0.0402 - val_loss: 0.0427 - 341ms/epoch - 9ms/step\n",
      "Epoch 25/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0438 - 332ms/epoch - 9ms/step\n",
      "Epoch 26/60\n",
      "39/39 - 0s - loss: 0.0418 - val_loss: 0.0427 - 333ms/epoch - 9ms/step\n",
      "Epoch 27/60\n",
      "39/39 - 0s - loss: 0.0379 - val_loss: 0.0431 - 308ms/epoch - 8ms/step\n",
      "Epoch 28/60\n",
      "39/39 - 0s - loss: 0.0396 - val_loss: 0.0428 - 333ms/epoch - 9ms/step\n",
      "Epoch 29/60\n",
      "39/39 - 0s - loss: 0.0383 - val_loss: 0.0427 - 340ms/epoch - 9ms/step\n",
      "Epoch 30/60\n",
      "39/39 - 0s - loss: 0.0400 - val_loss: 0.0433 - 328ms/epoch - 8ms/step\n",
      "Epoch 31/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0436 - 398ms/epoch - 10ms/step\n",
      "Epoch 32/60\n",
      "39/39 - 0s - loss: 0.0403 - val_loss: 0.0430 - 340ms/epoch - 9ms/step\n",
      "Epoch 33/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0428 - 391ms/epoch - 10ms/step\n",
      "Epoch 34/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0427 - 380ms/epoch - 10ms/step\n",
      "Epoch 35/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0432 - 384ms/epoch - 10ms/step\n",
      "Epoch 36/60\n",
      "39/39 - 0s - loss: 0.0395 - val_loss: 0.0435 - 349ms/epoch - 9ms/step\n",
      "Epoch 37/60\n",
      "39/39 - 0s - loss: 0.0397 - val_loss: 0.0437 - 436ms/epoch - 11ms/step\n",
      "Epoch 38/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0431 - 410ms/epoch - 11ms/step\n",
      "Epoch 39/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0430 - 387ms/epoch - 10ms/step\n",
      "Epoch 40/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0431 - 367ms/epoch - 9ms/step\n",
      "Epoch 41/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0431 - 338ms/epoch - 9ms/step\n",
      "Epoch 42/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0431 - 347ms/epoch - 9ms/step\n",
      "Epoch 43/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0434 - 362ms/epoch - 9ms/step\n",
      "Epoch 44/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0431 - 317ms/epoch - 8ms/step\n",
      "Epoch 45/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0430 - 308ms/epoch - 8ms/step\n",
      "Epoch 46/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0432 - 346ms/epoch - 9ms/step\n",
      "Epoch 47/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0432 - 373ms/epoch - 10ms/step\n",
      "Epoch 48/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0432 - 343ms/epoch - 9ms/step\n",
      "Epoch 49/60\n",
      "39/39 - 0s - loss: 0.0389 - val_loss: 0.0434 - 346ms/epoch - 9ms/step\n",
      "Epoch 50/60\n",
      "39/39 - 0s - loss: 0.0391 - val_loss: 0.0432 - 327ms/epoch - 8ms/step\n",
      "Epoch 51/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0430 - 353ms/epoch - 9ms/step\n",
      "Epoch 52/60\n",
      "39/39 - 0s - loss: 0.0385 - val_loss: 0.0431 - 334ms/epoch - 9ms/step\n",
      "Epoch 53/60\n",
      "39/39 - 0s - loss: 0.0386 - val_loss: 0.0432 - 324ms/epoch - 8ms/step\n",
      "Epoch 54/60\n",
      "39/39 - 0s - loss: 0.0388 - val_loss: 0.0434 - 305ms/epoch - 8ms/step\n",
      "Epoch 55/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0430 - 292ms/epoch - 7ms/step\n",
      "Epoch 56/60\n",
      "39/39 - 0s - loss: 0.0384 - val_loss: 0.0431 - 326ms/epoch - 8ms/step\n",
      "Epoch 57/60\n",
      "39/39 - 0s - loss: 0.0387 - val_loss: 0.0437 - 319ms/epoch - 8ms/step\n",
      "Epoch 58/60\n",
      "39/39 - 0s - loss: 0.0394 - val_loss: 0.0433 - 297ms/epoch - 8ms/step\n",
      "Epoch 59/60\n",
      "39/39 - 0s - loss: 0.0392 - val_loss: 0.0442 - 297ms/epoch - 8ms/step\n",
      "Epoch 60/60\n",
      "39/39 - 0s - loss: 0.0436 - val_loss: 0.0563 - 315ms/epoch - 8ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9 0.04232055468013068 [0.0039646408140614695, 89]\n",
      "最优学习率、LSTM层神经元的参数分别为： [0.0039646408140614695, 89]\n"
     ]
    }
   ],
   "source": [
    "trace,best,result=IPSO(train_X,train_y,test_X,test_y)\n",
    "savemat('结果/IPSO_para.mat',{'trace':trace,'best':best,'result':result})\n",
    "print(\"最优学习率、LSTM层神经元的参数分别为：\",[int(best[i]) if i>0 else best[i] for i in range(len(best))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a503697c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T02:34:32.276092Z",
     "start_time": "2023-05-07T02:34:30.875778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#迭代次数适应度函数曲线\n",
    "from matplotlib.pylab import mpl\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']   #显示中文\n",
    "mpl.rcParams['axes.unicode_minus']=False       #显示负号\n",
    "plt.figure(figsize=(6,4),dpi=500)\n",
    "plt.plot(trace,'r',linestyle=\"--\",linewidth=0.5)\n",
    "plt.xticks(list(range(0,31, 5)))\n",
    "plt.xlabel('迭代次数',fontsize=10)\n",
    "plt.ylabel('适应度值',fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c689c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
